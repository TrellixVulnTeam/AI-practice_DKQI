{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 思路\n",
    "1. 数据处理，构造数据集\n",
    "    - 文本：选max_vocab常见word、K个常见ans\n",
    "    - 图像：\n",
    "- 模型\n",
    "    - image：resize->cnn->fc_1024\n",
    "    - question：embeddin->rnn->fc_1024\n",
    "    - concat or add->fc_K->交叉熵\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import json\n",
    "with open('./dataset/mscoco_val2014_annotations.json') as f1, open('./dataset/MultipleChoice_mscoco_val2014_questions.json') as f2:\n",
    "    annotations = json.load(f1)\n",
    "    questions = json.load(f2) \n",
    "\n",
    "# with open('./dataset/v2_mscoco_val2014_annotations.json') as f1, open('./dataset/v2_OpenEnded_mscoco_val2014_questions.json') as f2:\n",
    "#     annotations = json.load(f1)\n",
    "#     questions = json.load(f2) \n",
    "# with open('./dataset/OpenEnded_mscoco_val2014_questions.json') as f:\n",
    "#     question = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[解析JSON](https://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p02_read-write_json_data.html)，先观察数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方API中vqaTools demo："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from vqaTool import VQA\n",
    "\n",
    "dataDir\t\t='./dataset'\n",
    "versionType ='' # this should be '' when using VQA v2.0 dataset\n",
    "taskType    ='MultipleChoice' # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
    "dataType    ='mscoco'  # 'mscoco' only for v1.0. 'mscoco' for real and 'abstract_v002' for abstract for v1.0.\n",
    "dataSubType ='val2014'\n",
    "annFile     ='%s/%s%s_%s_annotations.json'%(dataDir, versionType, dataType, dataSubType)\n",
    "quesFile    ='%s/%s%s_%s_%s_questions.json'%(dataDir, versionType, taskType, dataType, dataSubType)\n",
    "# imgDir \t\t= '%s/Images/%s/%s/' %(dataDir, dataType, dataSubType)\n",
    "img_dir = '%s/Images/%s/' %(dataDir, dataSubType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of ques: 121512\n"
     ]
    }
   ],
   "source": [
    "ann = annotations['annotations'][:]\n",
    "ques = questions['questions'][:]\n",
    "print('num of ques:', len(ques))\n",
    "\n",
    "QUES_N = len(ques) \n",
    "ANS_K = 1000 #最常见的ans数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_type': 'other',\n",
       " 'answers': [{'answer': 'tree', 'answer_confidence': 'yes', 'answer_id': 1},\n",
       "  {'answer': 'tree', 'answer_confidence': 'yes', 'answer_id': 2},\n",
       "  {'answer': 'tree', 'answer_confidence': 'yes', 'answer_id': 3},\n",
       "  {'answer': 'tree', 'answer_confidence': 'yes', 'answer_id': 4},\n",
       "  {'answer': 'tree', 'answer_confidence': 'yes', 'answer_id': 5},\n",
       "  {'answer': 'tree', 'answer_confidence': 'yes', 'answer_id': 6},\n",
       "  {'answer': 'tree', 'answer_confidence': 'yes', 'answer_id': 7},\n",
       "  {'answer': 'tree', 'answer_confidence': 'yes', 'answer_id': 8},\n",
       "  {'answer': 'tree', 'answer_confidence': 'yes', 'answer_id': 9},\n",
       "  {'answer': 'tree', 'answer_confidence': 'yes', 'answer_id': 10}],\n",
       " 'image_id': 8647,\n",
       " 'multiple_choice_answer': 'tree',\n",
       " 'question_id': 86471,\n",
       " 'question_type': 'what is in the'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'image_id': 8647,\n",
       " 'multiple_choices': ['red',\n",
       "  '4',\n",
       "  '2',\n",
       "  '1',\n",
       "  'berries and cake',\n",
       "  'light',\n",
       "  'no',\n",
       "  'sun',\n",
       "  'white',\n",
       "  'yes',\n",
       "  'green',\n",
       "  '1.50',\n",
       "  'mayonnaise',\n",
       "  '3',\n",
       "  'blue',\n",
       "  'tree',\n",
       "  'stamp',\n",
       "  'on feet'],\n",
       " 'question': 'What is in the top right corner?',\n",
       " 'question_id': 86471}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ann[5])\n",
    "display(ques[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造数据集\n",
    "做完基本处理后放dataloader里，可以并行  \n",
    "**（1）处理文本**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of images： 40504\n",
      "num of ans: 10908\n",
      "['wood', 'no', 'kettles']\n",
      "\n",
      "['What is the table made of?', 'Is the food napping on the table?', 'What has been upcycled to make lights?']\n",
      "\n",
      "[350623]\n"
     ]
    }
   ],
   "source": [
    "#提取ques ans imgId\n",
    "from collections import Counter\n",
    "ground_truth_trainset = []\n",
    "ques_trainset = []\n",
    "img_ids = []\n",
    "\n",
    "\n",
    "for i in range(QUES_N):\n",
    "    ground_truth_trainset.append(ann[i]['multiple_choice_answer'])\n",
    "    ques_trainset.append(ques[i]['question'])\n",
    "    if i % 3 == 0:\n",
    "        img_ids.append(ques[i]['image_id'])\n",
    "        \n",
    "print('num of images：', len(img_ids))\n",
    "ans_words = dict(Counter(ground_truth_trainset))\n",
    "print('num of ans:', len(ans_words))\n",
    "ans_words_most = dict(Counter(ground_truth_trainset).most_common(ANS_K))#最常见的1000个ans作分类\n",
    "\n",
    "print(ground_truth_trainset[:3], ques_trainset[:3], img_ids[:1], sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 19796\n",
      "num of ans: 10908\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en')\n",
    "# return nlp.tokenizer(text)\n",
    "\n",
    "import torch\n",
    "\n",
    "def word_tokenize(text):\n",
    "    t = [w for w in text.split(' ')]\n",
    "    t[-1] = t[-1][:-1]\n",
    "    return t\n",
    "\n",
    "word2idx = {'UNK':0}\n",
    "ans2idx = {}\n",
    "v, v_a = 1, 0\n",
    "\n",
    "for i in range(QUES_N):\n",
    "    for w in word_tokenize(ques_trainset[i]):\n",
    "        if w not in word2idx:\n",
    "            word2idx[w] = v\n",
    "            v += 1\n",
    "    ques_trainset[i] = torch.LongTensor([word2idx[w] for w in word_tokenize(ques_trainset[i])])\n",
    "\n",
    "for i,w in enumerate(ground_truth_trainset):\n",
    "    if w not in word2idx:\n",
    "        word2idx[w] = v\n",
    "        v += 1\n",
    "    if w not in ans2idx:\n",
    "        ans2idx[w] = v_a\n",
    "        v_a += 1\n",
    "    ground_truth_trainset[i] =torch.LongTensor([ans2idx[w]])\n",
    "\n",
    "print('vocab_size:',v)\n",
    "print('num of ans:',v_a)\n",
    "VOCAB_SIZE = v\n",
    "idx2word = {i:w for i,w in enumerate(word2idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句长mean:6.205726, max:23\n"
     ]
    }
   ],
   "source": [
    "s,m = 0,0\n",
    "for i,x in enumerate(ques_trainset):\n",
    "    s+=len(x)\n",
    "    if len(x)>m:\n",
    "        m=len(x)\n",
    "print('句长mean:%f, max:%d'%(s/i, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** （2）处理图像 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350623 torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07969069480895996\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "img_size = 256\n",
    "\n",
    "def trans_img(path, size=img_size):\n",
    "    mode = Image.open(path)\n",
    "    if len(mode.split()) == 3:\n",
    "        transform1 = transforms.Compose([\n",
    "            transforms.Resize((size, size)),#确保无黑边\n",
    "            transforms.CenterCrop((size, size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    else:\n",
    "        transform1 = transforms.Compose([\n",
    "            transforms.Resize((size, size)),#确保无黑边\n",
    "            transforms.CenterCrop((size, size)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    mode = transform1(mode)\n",
    "    return mode\n",
    "\n",
    "def show_img(image):\n",
    "    mode = transforms.ToPILImage()(image)\n",
    "    plt.imshow(mode)\n",
    "    plt.show()\n",
    "    \n",
    "a = time()\n",
    "for img_id in img_ids[:1]:\n",
    "    img_filename = 'COCO_' + dataSubType + '_'+ str(img_id).zfill(12) + '.jpg'\n",
    "    img = trans_img(img_dir + img_filename)\n",
    "    print(img_id, img.shape)\n",
    "    show_img(img)\n",
    "print(time()-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**（3）DataLoader**  \n",
    "后来发现可以写一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 1024 # 2:128, 3:92, 4:83, 5/1000:73/76, 50/8:64/66, \n",
    "MAX_SEQ_LEN = 23\n",
    "\n",
    "class VQA_Dataset(Dataset):\n",
    "    def __init__(self, img_ids, q, a):\n",
    "        self.img_ids = img_ids\n",
    "        self.q = q            \n",
    "        self.a = a\n",
    "        \n",
    "    def __len__(self):\n",
    "        return QUES_N\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = 'COCO_' + dataSubType + '_'+ str(img_ids[idx//3]).zfill(12) + '.jpg'\n",
    "        img = trans_img(img_dir + img_filename)\n",
    "        Lq = len(self.q[idx])\n",
    "        if Lq > MAX_SEQ_LEN:\n",
    "            self.q[idx] = self.q[idx][:MAX_SEQ_LEN]\n",
    "        elif Lq < MAX_SEQ_LEN:\n",
    "            self.q[idx] = F.pad(self.q[idx], (0,MAX_SEQ_LEN-Lq))#前后分别补多少\n",
    "        return img, self.q[idx], self.a[idx]\n",
    "    \n",
    "dataset = VQA_Dataset(img_ids=img_ids, q=ques_trainset, a=ground_truth_trainset)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=False, num_workers=40)\n",
    "# return list[img,q,a], img:[b,3,256,256], q:[b,seq], a:[b,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCH = 1\n",
    "\n",
    "class VQA_baseline(nn.Module):\n",
    "    def __init__(self, embed_dim, fc_dim, out_dim):\n",
    "        super(VQA_baseline, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 10, 5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.img_fc = nn.Linear(in_features=10*61*61, out_features=fc_dim)\n",
    "        \n",
    "        self.embed = nn.Embedding(VOCAB_SIZE, embed_dim)\n",
    "        self.ques_fc = nn.Linear(MAX_SEQ_LEN*embed_dim, fc_dim)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self.fc2ans = nn.Linear(fc_dim, out_dim)\n",
    "        \n",
    "    def forward(self, p, q):\n",
    "        # pic:[b,3,256,256]\n",
    "        p = self.pool(F.relu(self.conv1(p))) #[b,6,252,252]->[b,6,126,126]\n",
    "        p = self.pool(F.relu(self.conv2(p))) #[b,10,122,122]->[b,10,61,61]\n",
    "        p = p.view(-1, 10*61*61) #[b,10*61*61]\n",
    "        p = self.bn(self.img_fc(p)) #[b,fc_dim]\n",
    "        \n",
    "        # ques:[b,seq]\n",
    "        q = q.view(MAX_SEQ_LEN, -1)#[seq,b]，因为并行时，batch_size会被打开\n",
    "        q = self.embed(q)#[seq,emb,b]\n",
    "        q = q.view(-1, MAX_SEQ_LEN * self.embed_dim)\n",
    "        \n",
    "        q = self.ques_fc(q)\n",
    "        q = self.bn(q)#[b,fc_dim]\n",
    "        \n",
    "        return self.fc2ans(p+q) #element-wise add, [b,out_dim]\n",
    "      \n",
    "        \n",
    "model = VQA_baseline(embed_dim=300, fc_dim=1024, out_dim=10908)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    device_ids = [0,1,2,3]\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "t = time()\n",
    "for epoch in range(EPOCH):\n",
    "    for i, (img, q, a) in enumerate(dataloader):\n",
    "        opt.zero_grad()\n",
    "        if USE_CUDA:\n",
    "            img,q,a = img.cuda(),q.cuda(),a.cuda()\n",
    "        \n",
    "        y = model(img, q)\n",
    "        loss = loss_fn(y, a.squeeze())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print('epoch:',epoch+1,'iter:',i,'loss:',loss.item())\n",
    "            print(time() - t)\n",
    "            t = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
