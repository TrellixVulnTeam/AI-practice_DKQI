{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/data/train_set.csv') as f:\n",
    "    train = f.readlines()[1:]\n",
    "# test = pd.read_csv('data/test_a.csv')\n",
    "with open('dataset/data/test_b.csv') as f:\n",
    "    test = f.readlines()[1:]\n",
    "    x_test = []\n",
    "    for s in test:\n",
    "        x_test.append(s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "907.20711 57921 2 996.0265463156331\n"
     ]
    }
   ],
   "source": [
    "x, y = [], []\n",
    "L = []\n",
    "category = set()\n",
    "\n",
    "for line in train:\n",
    "    y_, x_ = line.split('\\t')\n",
    "    category.add(int(y_))\n",
    "    x.append(x_.strip())\n",
    "    y.append(y_.strip())\n",
    "    L.append(len(x_.split()))\n",
    "    \n",
    "# with open('dataset/data/class.txt','w') as f:\n",
    "#     for c in sorted(category):\n",
    "#         f.write(str(c)+'\\n')\n",
    "print(len(y))\n",
    "print(np.mean(L), np.max(L), np.min(L), np.std(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500长度可以覆盖 百分之36.24450样本\n",
      "1000长度可以覆盖 百分之69.41350样本\n",
      "2000长度可以覆盖 百分之92.02250样本\n",
      "3000长度可以覆盖 百分之97.11500样本\n",
      "4000长度可以覆盖 百分之98.79400样本\n",
      "5000长度可以覆盖 百分之99.44150样本\n"
     ]
    }
   ],
   "source": [
    "def C(k):\n",
    "    i=0\n",
    "    for s in L:\n",
    "        if s<=k:\n",
    "            i+=1\n",
    "    print('%d长度可以覆盖 百分之%.5f样本'%(k,i/len(L)*100))\n",
    "C(500)\n",
    "C(1000)\n",
    "C(2000)\n",
    "C(3000)\n",
    "C(4000)\n",
    "C(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "#train_all\n",
    "with open('dataset/data/train_all.txt','w') as f:\n",
    "    for i in range(len(y_train)):\n",
    "        f.write(x_train[i]+'\\t'+y_train[i]+'\\n')\n",
    "    for i in range(len(y_dev)):\n",
    "        f.write(x_dev[i]+'\\t'+y_dev[i]+'\\n')\n",
    "\n",
    "# train dev test\n",
    "# with open('dataset/data/train.txt','w') as f:\n",
    "#     for i in range(len(y_train)):\n",
    "#         f.write(x_train[i]+'\\t'+y_train[i]+'\\n')        \n",
    "# with open('dataset/data/dev.txt','w') as f:\n",
    "#     for i in range(len(y_dev)):\n",
    "#         f.write(x_dev[i]+'\\t'+y_dev[i]+'\\n')\n",
    "# with open('dataset/data/test.txt','w') as f:\n",
    "#     for i in range(len(test)):\n",
    "#         f.write(test[i])\n",
    "\n",
    "#corpus\n",
    "# with open('dataset/data/corpus.txt','w') as f:\n",
    "#     for i in range(len(y_train)):\n",
    "#         f.write(x_train[i]+'\\n')\n",
    "#     for i in range(len(y_dev)):\n",
    "#         f.write(x_dev[i]+'\\n')    \n",
    "#     for i in range(len(test)):\n",
    "#         f.write(test[i])\n",
    "with open('dataset/data/train_all.txt','r') as f:\n",
    "    f = f.readlines()\n",
    "print(len(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cython\n",
    "import numpy as np\n",
    "from gensim.models import word2vec #必须按照[[],[]]才能保留原词！\n",
    "from gensim.models.word2vec import LineSentence\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import svm\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.externals import joblib\n",
    "# from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.61 s, sys: 9.82 s, total: 19.4 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_size = 300\n",
    "X = []\n",
    "with open('dataset/data/corpus.txt', encoding='utf-8') as f:\n",
    "    lines = f.readlines()[:] \n",
    "    X = [[] for i in range(len(lines))]\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        X[i] = line.split(' ')#由于这一整个是str，但是w2v要一个个词\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6151, size=300, alpha=0.025)\n",
      "CPU times: user 43.3 s, sys: 9.41 s, total: 52.7 s\n",
      "Wall time: 51.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k=None\n",
    "model = word2vec.Word2Vec(X[:k], size=w2v_size, min_count=5, workers=-1, iter=10)\n",
    "print(model)\n",
    "# model.build_vocab(X[:k])  # prepare the model vocabulary\n",
    "# model.train(X[:k], total_examples=len(X[:k]), epochs=model.iter)\n",
    "\n",
    "model.save('dataset/data/w2v/w2v_300_mincount5.model')\n",
    "    # model.wv.save_word2vec_format(\"444.model.bin\", binary=True)\n",
    "    #增量学习 incremental/online learning\n",
    "    # model = word2vec.Word2Vec.load('w2v.model')\n",
    "    #model = word2vec.Word2Vec.load_word2vec_format(\"444.model.bin\", binary=True)\n",
    "    #model.build_vocab(X, update=True) #其实文档里都有的，虽然没有明确标出\n",
    "    #model.train(X, total_examples=len(X), epochs=model.iter)\n",
    "\n",
    "    # print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5981, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6871, 256)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dataset/data/bert_emb.npy','rb') as f:\n",
    "    bert_emb = np.load(f)\n",
    "with open('dataset/bert-mini/vocab.txt') as f:\n",
    "    vocab_bert = {w[:-1]:i for i,w in enumerate(f.readlines())}\n",
    "print(bert_emb.shape)\n",
    "\n",
    "V = pkl.load(open('dataset/data/vocab_tra_all.pkl','rb'))\n",
    "emb = np.zeros((len(V), 256)) #* 0.000962582706903456\n",
    "for w in V.keys():\n",
    "    if w in vocab_bert:\n",
    "        emb[V[w]] = bert_emb[vocab_bert[w]]\n",
    "\n",
    "np.save('dataset/data/bert_6871_256.npy',emb)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6151, size=300, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.9378990724951244e-05, 0.09832080340287651)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"dataset/data/w2v/w2v_300_mincount5.model\")\n",
    "print(model)\n",
    "\n",
    "V = pkl.load(open('dataset/data/vocab.pkl','rb'))\n",
    "# for word in model.wv.vocab:\n",
    "#     print(word)\n",
    "emb = np.random.randn(len(V), 300) #* 0.000962582706903456\n",
    "for w in V.keys():\n",
    "    if w in model.wv.vocab:\n",
    "        emb[V[w],:] = model.wv[w] * 100#正常的emb大概是1~2个小数点\n",
    "#     break\n",
    "emb.mean(), emb.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6979, 300)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save('dataset/data/w2v_300.npy',emb)\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# import importlib\n",
    "# import time\n",
    "# import numpy as np\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer  # 计算tfidf\n",
    "# from sklearn.feature_extraction.text import CountVectorizer  # 计算df\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer  # “一步到位”\n",
    "# from gensim.models import word2vec\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn import svm\n",
    "# from sklearn.linear_model.logistic import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn import tree\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.externals import joblib\n",
    "# from sklearn import metrics\n",
    "\n",
    "# id_num = []\n",
    "# word_article = []\n",
    "# words_article = []\n",
    "# label = []\n",
    "# X_train = []\n",
    "# y_train = []\n",
    "# X_test = []\n",
    "# y_test = []\n",
    "\n",
    "# load_start = time.time()\n",
    "# importlib.reload(sys)\n",
    "# with open('E:/&数据集/DC竞赛数据集/train_set.csv', 'r', encoding='utf-8') as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines[1:20000]:\n",
    "#         line = line.split(',')\n",
    "#         X_train.append(line[2])\n",
    "#         y_train.append(line[3])\n",
    "#     for line in lines[20000:22000]:  # 划分出测试集\n",
    "#         line = line.split(',')\n",
    "#         X_test.append(line[2])\n",
    "#         y_test.append(line[3])\n",
    "# print(\"Time consumption on loading:\", time.time() - load_start)\n",
    "\n",
    "# train_start = time.time()\n",
    "# w2v_size = 256\n",
    "# w2v_model = word2vec.Word2Vec.load('./w2v/w2v.model')\n",
    "\n",
    "# def tran_X(X_list):\n",
    "#     X = [[] for i in range(len(X_list))]\n",
    "#     for i in range(0, len(X_list)):\n",
    "#         j = X_list[i].split(' ')  # 把str分成词list\n",
    "#         X[i] += j\n",
    "#     # print(X)\n",
    "#     X_vectors = np.zeros((len(X), w2v_size))\n",
    "#     i = 0\n",
    "#     for words in X:\n",
    "#         j = 0\n",
    "#         for word in words:\n",
    "#             if word in w2v_model.wv.vocab:\n",
    "#                 X_vectors[i] += w2v_model[word]\n",
    "#                 j += 1\n",
    "#         X_vectors[i] /= j\n",
    "#         i += 1\n",
    "#     print(\"shape:\", X_vectors.shape)\n",
    "\n",
    "#     for i in range(len(X_vectors)):\n",
    "#         for j in range(len(X_vectors[i])):\n",
    "#             if np.isnan(X_vectors[i][j]):  # 去除NaN\n",
    "#                 X_vectors[i][j] = 0\n",
    "#     # X_vectors = preprocessing.scale(X_vectors) #标准化\n",
    "#     X_vectors = preprocessing.MinMaxScaler().fit_transform(X_vectors)  # 归一化\n",
    "#     # model_vectors = np.transpose(model_vectors) #转置\n",
    "#     return X_vectors\n",
    "\n",
    "X_train = tran_X(X_train)\n",
    "print(\"Time consumption on training:\", time.time() - train_start)\n",
    "clf = svm.LinearSVC(C=1.2)\n",
    "clf.fit(X_train, y_train)\n",
    "# joblib.dump(clf, 'lineSVM_20000__.m')\n",
    "print(\"Time consumption on training:\", time.time() - train_start)\n",
    "\n",
    "test_start = time.time()\n",
    "# clf = joblib.load('lineSVM_20000__.m')\n",
    "prediction = clf.predict(X_train)  # 训练集上准确率\n",
    "print(metrics.classification_report(y_train, prediction))\n",
    "X_test = tran_X(X_test)\n",
    "prediction = clf.predict(X_test)  # 测试集上准确率\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "# print(metrics.confusion_matrix(y_test,prediction))\n",
    "print(\"Time consumption on testing:\", time.time() - test_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 传统方法(TF-IDF+SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 4), norm='l2', preprocessor=None, smooth_idf=1,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=1,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=1,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer(ngram_range=(1,4), min_df=3, max_df=0.9, use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "vect.fit(x_train+x_dev+x_test) \n",
    "# x_train = vect.transform(x_train[:])\n",
    "# X_test = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 19269679)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(vect, open(\"dataset/tfidf_vect.pickle\", \"wb\"))\n",
    "vect = pickle.load(open(\"dataset/tfidf_vect.pickle\", \"rb\"))\n",
    "vect.transform(x_train[:]).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 47s, sys: 23.9 s, total: 4min 11s\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_ = vect.transform(x_train[:])\n",
    "x_dev_ = vect.transform(x_dev[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9451575162448084\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lsvc = svm.LinearSVC() #0.94516\n",
    "sgdc = SGDClassifier(loss=\"modified_huber\", alpha=0.000008, n_jobs=-1)\n",
    "rf = RandomForestClassifier(n_jobs=-1) #0.71\n",
    "\n",
    "clf = rf\n",
    "\n",
    "k = None\n",
    "clf.fit(x_train_[:k], y_train[:k])\n",
    "\n",
    "pred = clf.predict(x_dev_)\n",
    "dev_f1 = f1_score(pred, y_dev, average='macro')\n",
    "print(dev_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9420985068018918\n"
     ]
    }
   ],
   "source": [
    "with open('results/%s_result_%.4f.csv'%('TFIDF+ML', dev_f1), 'w') as f:\n",
    "    f.write('label\\n')\n",
    "    for p in clf.predict(x_test_):\n",
    "        f.write(str(p)+'\\n')\n",
    "        \n",
    "prob = clf.predict_proba(x_dev_)\n",
    "np.save('results/TFIDF+ML_prob', prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "# from sklearn.linear_model.logistic import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier  \n",
    "# from sklearn.neighbors import KNeighborsClassifier  \n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.externals import joblib\n",
    "# from sklearn import metrics\n",
    " \n",
    "# def training(train_set_in):\n",
    "#     X = get_tfidf(train_set_in)\n",
    "#     Y = []\n",
    "#     for sente in cut_sentence(train_set_in):\n",
    "#         Y.append(sente[0:1])\n",
    "#     svc=svm.SVC(C=1,kernel='poly',degree=3,gamma=10,coef0=0) #选择模型 & 参数\n",
    "#     lr = LogisticRegression()   \n",
    "#     knn = KNeighborsClassifier()  \n",
    "#     dt = tree.DecisionTreeClassifier()\n",
    "    \n",
    "#     rf = RandomForestClassifier()\n",
    "# #     multi_target_forest = MultiOutputClassifier(rf)  # 构建多输出多分类器\n",
    "# #     clf = multi_target_forest.fit(X, Y)\n",
    "#     sgdc = SGDClassifier(loss=\"modified_huber\", penalty=\"l2\", alpha=0.00002, n_jobs=-1)# modified_huber; penalty=\"elasticnet\"\n",
    "# #     pre = clf.predict(X)\n",
    "# #     print('多输出多分类器预测输出分类:\\n',y_pred)\n",
    "    \n",
    "#     clf = sgdc.fit(X,Y) #训练模型\n",
    "#     joblib.dump(clf,'train_model.m') #保存模型\n",
    "    \n",
    "# def testing(test_set_in):\n",
    "#     file_original_txt = open(test_set_in,'r',encoding='utf-8')\n",
    "#     original_txt = file_original_txt.readlines() #原始文本\n",
    "#     i,j = 0,0\n",
    "#     clf = joblib.load('train_model.m') #加载模型   \n",
    "#     for sente in cut_sentence(test_set_in):\n",
    "#         label = int(sente[0:1])\n",
    "#         T = [] \n",
    "#         T.append(str(sente[2:]))      \n",
    "#         tf=vectorizer.transform(T) #将数据进行转换，比如数据的归一化和标准化，将测试数据按照训练数据同样的模型进行转换，得到特征向量。\n",
    "#         tfidf=transformer.transform(tf)\n",
    "#         print(label,'\\t',int(clf.predict(tfidf)),'\\t',str(original_txt[j][2:])) \n",
    "#         if label == int(clf.predict(tfidf)):\n",
    "#             i += 1 \n",
    "#         j += 1  \n",
    "#     print('\\nprecision :',i/j)\n",
    "    \n",
    "# training('./project/dataset/train_set0.txt')\n",
    "# testing('./project/dataset/test_set0.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from importlib import import_module\n",
    "from utils import build_dataset, build_iterator, get_time_dif\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "\n",
    "dataset = 'dataset'  # 数据集THUCNews\n",
    "embedding = 'random'  # 'embedding_SougouNews.npz'  'random'\n",
    "use_word = True  # 中文用char更好，不会有OOV+学习char分布比word分布更容易(因为量多）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_model(model_names):\n",
    "    ret = []\n",
    "    for model_name in model_names:\n",
    "        x = import_module(name='models.' + model_name)  # 绝对导入\n",
    "        config = x.Config(dataset, embedding)  # Config类\n",
    "        config.train_path = dataset + '/data/dev.txt'\n",
    "        vocab, train_data, dev_data, test_data = build_dataset(config, use_word)\n",
    "        dev_iter = build_iterator(dev_data, config)\n",
    "        test_iter = build_iterator(test_data, config)  # an object\n",
    "        config.n_vocab = len(vocab)\n",
    "        config.device = torch.device('cuda:0')\n",
    "        model = x.Model(config)#.to(config.device)\n",
    "        ret.append([model, config, dev_iter, test_iter])\n",
    "    return ret\n",
    "\n",
    "models = get_model(['TextRNN','TextCNN'])# TextCNN, TextRNN, FastText, TextRCNN, TextRNN_Att, DPCNN, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0][0].load_state_dict(torch.load('dataset/saved_dict/TextRNN.ckpt_0.936358_59367.ckpt', map_location='cuda:0'))\n",
    "models[1][0].load_state_dict(torch.load('dataset/saved_dict/TextCNN.ckpt_0.937892_74445.ckpt', map_location='cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proba(models):\n",
    "    ret = []\n",
    "    # for model in models:\n",
    "    m1, m2 = models\n",
    "    model1, config, dev_iter, test_iter = m1\n",
    "    model2, config, dev_iter, test_iter = m2\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dev_iter:\n",
    "            outputs = model1(texts) + model2(texts)\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = torch.max(outputs.data, 1)[1].cpu().numpy()\n",
    "            labels_all = np.append(labels_all, labels)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    f1 = f1_score(labels_all, predict_all, average='macro')\n",
    "    print(f1)\n",
    "    report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(report)\n",
    "    ret.append([])\n",
    "get_proba(models)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
