{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from time import time\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "\n",
    "MAX_VOCAB_SIZE = 100000\n",
    "EMBEDDING_SIZE = 80\n",
    "HIDDEN_SIZE = 80\n",
    "BATCH_SIZE = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.torchtext for preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取Field 以及训练LM的数据集格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = torchtext.data.Field(lower=True) #变小写\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "                   path=\"./dataset\", text_field=text,\n",
    "                   train=\"new_trainset_havestp_C.txt\", \n",
    "                   validation=\"new_testset_havestp_C.txt\", \n",
    "                   test=\"new_testset_havestp_C.txt\" )\n",
    "\n",
    "text.build_vocab(train, max_size=MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词数： 100002\n",
      "['<unk>', '<pad>', '的', '了', '在', '是', '和', '有', '也', '不']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('词数：',len(text.vocab))\n",
    "print(text.vocab.itos[:10] )#跟C++ 11用法一样\n",
    "\n",
    "# 一个编程的好习惯，及时test\n",
    "assert text.vocab.itos[:10] == ['<unk>', '<pad>', '的', '了', '在', '是', '和', '有', '也', '不']\n",
    "text.vocab.stoi['<unk>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=BATCH_SIZE, device=-1, \n",
    "    bptt_len=20, repeat=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.iterator.BPTTIterator object at 0x0000023BC473CBA8>\n",
      "<generator object BPTTIterator.__iter__ at 0x0000023BCD879F10>\n",
      "\n",
      "[torchtext.data.batch.Batch of size 35]\n",
      "\t[.text]:[torch.LongTensor of size 20x35]\n",
      "\t[.target]:[torch.LongTensor of size 20x35]\n"
     ]
    }
   ],
   "source": [
    "print(train_iter)\n",
    "print(iter(train_iter)) #迭代器，可以理解为一种指针？\n",
    "it = iter(train_iter)\n",
    "print(next(it)) #返回iter的下一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 35]) torch.Size([20, 35])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1645,  2408,   363,    78,   698,   917,  5940,   794,  1705,  5940,\n",
       "          794,  3537,  1411, 16840,  2478,    68,    18,   163,  5106,   917])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(it)\n",
    "print(batch.text.shape, batch.target.shape) # bptt_len * batch_size，我们更习惯反过来\n",
    "batch.text[:,0] # 因为第一个维度是“句”长，so一列是 一句话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "签到卡张<eos>哪里回收翡翠啊有一个翡翠手镯卖那里<unk>的啊<eos>实用高层建筑火灾\n",
      "卡张<eos>哪里回收翡翠啊有一个翡翠手镯卖那里<unk>的啊<eos>实用高层建筑火灾应该 \n",
      "\n",
      "应该如何逃生年月日举世闻名的美国纽约世贸中心能容纳五万<unk>的姐妹楼在遭受恐怖分子袭击后\n",
      "如何逃生年月日举世闻名的美国纽约世贸中心能容纳五万<unk>的姐妹楼在遭受恐怖分子袭击后相继 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    batch = next(it)\n",
    "    print(''.join([text.vocab.itos[j] for j in batch.text[:,0]]))\n",
    "    print(''.join([text.vocab.itos[j] for j in batch.target[:,0]]),'\\n')\n",
    "    # 所以虽是不同的batch，LSTM的 hidden可以一直传下去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(RNN_LM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, text, hidden):\n",
    "        # text: seq_len * batch_size\n",
    "        emb = self.embed(text) # seq_len * embed_size * batch_size\n",
    "        lstm_out, hidden = self.lstm(emb, hidden) # y, 末尾hidden\n",
    "        # lstm_out: seq_len * batch_size * hidden_size\n",
    "        # hidden: 1 * batch_size * hidden_size, 1 * batch_size * hidden_size\n",
    "        \n",
    "        vocab_out = self.linear(lstm_out.view(-1, lstm_out.shape[2])) \n",
    "        #(seq_len * batch_size) * hidden_size 变成2维\n",
    "        vocab_out = vocab_out.view(lstm_out.size(0),lstm_out.size(1), self.vocab_size) \n",
    "        #恢复seq_len * batch_size * vocab_size\n",
    "        return vocab_out, hidden #hidden也包含了信息\n",
    "    \n",
    "    def init_hidden(self, batch_size, requires_grad=True):\n",
    "        weight = next(self.parameters())\n",
    "        #返回：两个和weight一样的全0矩阵，cell 和 hidden\n",
    "        initrange = 1/(self.embed_size+self.hidden_size)\n",
    "        cell_w = weight.new_zeros(1,batch_size, self.hidden_size, requires_grad=requires_grad)\n",
    "        hidden_w = weight.new_zeros(1,batch_size, self.hidden_size, requires_grad=requires_grad)\n",
    "        with torch.no_grad():\n",
    "            return (cell_w.uniform_(-initrange, initrange),hidden_w.uniform_(-initrange, initrange))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_LM(\n",
       "  (embed): Embedding(100002, 80)\n",
       "  (lstm): LSTM(80, 80)\n",
       "  (linear): Linear(in_features=80, out_features=100002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN_LM(vocab_size=len(text.vocab),\n",
    "               embed_size=EMBEDDING_SIZE,\n",
    "               hidden_size=HIDDEN_SIZE)\n",
    "USE_CUDA = 1 #52 vs 232 per 100iters\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.5256, -0.7502, -0.6540,  ..., -0.5601,  0.3956, -0.9823],\n",
       "        [-0.5065,  0.0998, -0.6540,  ...,  1.8550, -0.7064,  2.5571],\n",
       "        [ 0.4175, -0.2127, -0.8400,  ..., -2.3648, -0.9295,  0.2936],\n",
       "        ...,\n",
       "        [-1.7138,  0.2265,  1.7561,  ...,  1.0764, -0.2923, -0.4635],\n",
       "        [ 0.9190,  1.1605, -0.3112,  ...,  0.7747,  0.4566,  0.0675],\n",
       "        [-0.9111, -0.2605, -0.5237,  ...,  0.4181, -0.0922, -0.3103]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()) #都在cuda上了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach() #计算图在此截断，否则相当于非常长的一样计算图\n",
    "        # 其实也相当于copy啊？\n",
    "    else:\n",
    "        return tuple(repackage_hidden(i) for i in h)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 5e-4\n",
    "optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 先定义评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    it = iter(data)\n",
    "    total_count = 0.\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
    "        for i, batch in enumerate(it):\n",
    "            data, target = batch.text, batch.target\n",
    "            if USE_CUDA:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model(data, hidden)\n",
    "            loss = loss_fn(output.view(-1, len(text.vocab)), target.view(-1))\n",
    "            total_count += np.multiply(*data.size())\n",
    "            total_loss += loss.item()*np.multiply(*data.size())\n",
    "            \n",
    "    loss = total_loss / total_count\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0  loss:11.5202 perplexity:100726.63  耗时:1.91s\n",
      "iter: 10  loss:11.5042 perplexity:99127.37  耗时:6.12s\n",
      "iter: 20  loss:11.4953 perplexity:98256.30  耗时:6.10s\n",
      "iter: 30  loss:11.4642 perplexity:95242.44  耗时:6.08s\n",
      "iter: 40  loss:11.4098 perplexity:90201.94  耗时:6.07s\n",
      "iter: 50  loss:11.2763 perplexity:78925.41  耗时:6.10s\n",
      "iter: 60  loss:10.6479 perplexity:42104.59  耗时:6.11s\n",
      "iter: 70  loss:9.9231 perplexity:20395.86  耗时:6.11s\n",
      "iter: 80  loss:9.5310 perplexity:13780.88  耗时:6.09s\n",
      "iter: 90  loss:9.0174 perplexity:8245.60  耗时:6.10s\n",
      "iter: 130  loss:8.9963 perplexity:8073.13  耗时:6.08s\n",
      "iter: 140  loss:8.7881 perplexity:6555.48  耗时:6.10s\n",
      "iter: 210  loss:8.7059 perplexity:6038.43  耗时:6.10s\n",
      "iter: 320  loss:8.6995 perplexity:5999.96  耗时:6.11s\n",
      "iter: 360  loss:8.5742 perplexity:5293.56  耗时:6.12s\n",
      "iter: 520  loss:8.5093 perplexity:4960.92  耗时:6.10s\n",
      "iter: 670  loss:8.4285 perplexity:4575.71  耗时:6.08s\n",
      "iter: 1060  loss:8.4278 perplexity:4572.45  耗时:6.08s\n",
      "iter: 1770  loss:8.3761 perplexity:4342.21  耗时:6.11s\n",
      "iter: 2490  loss:8.3598 perplexity:4271.70  耗时:6.13s\n",
      "iter: 2850  loss:8.3046 perplexity:4042.55  耗时:6.08s\n",
      "iter: 2920  loss:8.2265 perplexity:3738.82  耗时:6.08s\n",
      "iter: 3080  loss:8.0650 perplexity:3181.06  耗时:6.11s\n",
      "iter: 3560  loss:8.0395 perplexity:3101.00  耗时:6.10s\n",
      "iter: 4410  loss:7.9974 perplexity:2973.11  耗时:6.13s\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "GRAD_CLIP = 5.0\n",
    "min_loss = float(\"inf\")\n",
    "a = time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()#训练和测试不一样的，但是为啥之前都没写？\n",
    "    it = iter(train_iter)\n",
    "    hidden = model.init_hidden(BATCH_SIZE) #两个\n",
    "#     print(hidden[0].shape)\n",
    "    for i, batch in enumerate(it):\n",
    "        \n",
    "            \n",
    "        data, target = batch.text, batch.target\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "#         print(data.shape, target.shape)\n",
    "        hidden = repackage_hidden(hidden) #构建新的hidden起点\n",
    "        output, hidden = model(data, hidden)\n",
    "        \n",
    "        loss = loss_fn(output.view(-1,len(text.vocab)), target.view(-1)) \n",
    "        #output: batch_size * target_dim, \n",
    "        #target: batch_size *1 即可\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        optim.step()     \n",
    "        \n",
    "        if i%10 ==0:\n",
    "            if loss.item()<min_loss:\n",
    "                print(\"iter:\", i, ' loss:%.4f'%loss.item(), 'perplexity:%.2f'%np.exp(loss.item()), ' 耗时:%.2fs'%(time()-a))\n",
    "                min_loss = loss.item()\n",
    "            a = time()\n",
    "            \n",
    "#         if i>=4000:\n",
    "#             break\n",
    "#         if i%2 == 0:\n",
    "#             val_loss = evaluate(model, val_iter)\n",
    "#             if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "#                 print(\"better model, val loss: \", val_loss)\n",
    "#                 torch.save(model.state_dict(), \"LSTM%s.pth\"%i)\n",
    "#             else:\n",
    "#                 scheduler.step()\n",
    "#                 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#             val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/LSTM3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "循着 儿科医院 板凳 足总杯 加装 妻女 名 动植物 间接 他人 也 无 第三节 庆典 刘 遍 新型 彻底改变 的 公路 且 规划 安全 不 在 年间 或 发明 关注 到 很难 的 方法 运动 存在 举证 的 骗局 制定 同时 多 <unk> 将 重要 引导 相同 他们 以 增多 下 对 电网 的 达到 检查 其中 及 自身 都 有 激动 不远 的 位置 放在心上 各色 纪律 大家 事发 而 实名 拓宽 从 秒 就是 回国 暂停 手机 于 先进分子 的 蛇口 这 仇恨 登上 的 过程 建设 抗日 需要 郭氏 钱 符合要求 均 贵州 的 富豪 一站式 和 存款\n"
     ]
    }
   ],
   "source": [
    "hidden = best_model.init_hidden(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input = torch.randint(len(text.vocab), (1, 1), dtype=torch.long).to(device)\n",
    "words = []\n",
    "for i in range(100):\n",
    "    output, hidden = best_model(input, hidden)\n",
    "    word_weights = output.squeeze().exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    input.fill_(word_idx)\n",
    "    word = text.vocab.itos[word_idx]\n",
    "    words.append(word)\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
