CMU-MOSEI收集的数据来自YouTube的独白视频，并且去掉了那些包含过多人物的视频。
最终的数据集包含3228个视频，23453个句子，1000个讲述者，250个话题，总时长达到65小时。
数据集既有情感标注又有情绪标注。
情感标注是对每句话的7分类的情感标注，作者还提供了了2/5/7分类的标注。
情绪标注是包含高兴，悲伤，生气，恐惧，厌恶，惊讶六个方面的情绪标注


transformer 传统的
论文中的
加入了三个点：
（1）joint-encoding（联合编码）：文本和音频  （用bert编码替换glove（词向量）编码）
（2）modular co-attention（模块化协同注意力编码层）  VQA思路的迁移
    VQA思路：
    最近的研究表明，同时学习视觉和文本模式的共同注意，可以有利于图像和问题的细粒度表示，从而实现更准确的预测。然而，这些共同注意模型学习了多模态实例的粗糙交互，而所学习的共同注意不能推断出每个图像区域和每个问题词之间的相关性。这导致了这些共注意模型的显著局限性。
为了克服多模式交互作用不足的问题，已经提出了两个密集的共同注意模型BAN和DCN来建模任何图像区域和任何疑问词之间的密集交互作用。 密集的共同注意机制有助于理解图像问题关系，以正确回答问题。 有趣的是，这两个密集的共同注意模型都可以在深度上级联，形成支持更复杂的视觉推理的深度共同注意模型，从而有可能改善VQA性能。 但是，这些深层模型相对于其对应的浅层对应模型或粗糙的共同注意模型MFH [33]而言，几乎没有改善。 我们认为这些深度共同注意模型的瓶颈在于在每个模态中同时建模密集的自我注意（即问题的词对词关系和图像的区域对区域关系）的缺陷。
协同注意力模型（Co-Attention Models）
同时学习问题的文本注意力和图像的诗句注意力是有必要的，目前协同注意力网络网络是在每个模态中分别学习其注意力分布，且忽视了图像和文本的密集交互（dense attention）。这对多模态特征之间细粒度关系的理解造成了瓶颈。解决这一问题目前主要办法则是用密集的协同注意力网络。

Modular Co-Attention Layer试图解决上述问题（对其解释：https://blog.csdn.net/bxg1065283526/article/details/104820450）
（3）
glimpse layer at the end of each block


（1）bert编码替换
