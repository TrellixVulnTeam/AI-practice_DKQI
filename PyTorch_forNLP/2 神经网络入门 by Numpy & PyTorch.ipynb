{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建两层NN\n",
    "- $h = W_1X$ \n",
    "- $h\\_relu = max(0, h)$\n",
    "- $y_{hat} = W_2h\\_relu$\n",
    "\n",
    "<br>\n",
    "### 实践中发现：\n",
    "1. randn效果比rand好，最后的loss会收敛到更小----\n",
    "    - 如果xy也这么生成，效果会perfect，因为服从高斯分布\n",
    "+ lr影响非常之大，动不动就爆炸或消失\n",
    "+ 复杂数据，要做数据缩放才能收敛；然后我发现loss震荡，把hidden_neuron调到500就好了，让其尽情过拟合吧\n",
    "+ 自己电脑GPU更慢，可能是网络本身简单，交互都很费事了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.numpy版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 1\n",
      "139345.04711526804 286.061484500087 286.061484500087 2720.124235949752\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n",
      "0.5 0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array([[1,1],[2,1],[1,2],[2,2]])\n",
    "y = np.array([0,1,1,0,]).reshape(-1,1)\n",
    "\n",
    "N, D_in, D_out = X.shape[0], X.shape[1], y.shape[1] #N ：样本数，D_in：特征维度\n",
    "H = 500 #隐藏层神经元数\n",
    "print(N, D_in, D_out)\n",
    "\n",
    "w1 = np.random.rand(D_in, H) #w1 = np.zeros((D_in, H))\n",
    "w2 = np.random.rand(H, D_out) #w2 = np.zeros((H, D_out))\n",
    "\n",
    "learing_rate = 1e-2\n",
    "EPOCH = 500\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    # Forword pass\n",
    "    h = X.dot(w1)# N*D_in D_in*H -> N*H\n",
    "    h_relu = np.maximum(0, h)# N*H...max函数是选某维度的max\n",
    "    y_pred = h_relu.dot(w2)# N*H H*D_out -> N*D_out\n",
    "    \n",
    "    # Compute loss（MSE）\n",
    "    loss = np.square(y_pred - y).mean()#sum成标量\n",
    "\n",
    "    # Backward pass,compute gradient；即loss对参数求偏导，\n",
    "    # 因为是标量对向量/矩阵求，所以维度和向量一样就好。最后在纸上写出来\n",
    "    grad_y_pred = 2.0 * (y_pred - y) # N*D_out；因为是对y_pred求导，所以是y_pred-y\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)#H*D_out <- H*N N*D_out\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) #N*H <- N*D_out D_out*H\n",
    "    grad_h = grad_h_relu.copy() #浅拷贝，空间不同，但子对象相同。一般来说不能用赋值，否则可能改变被赋值对象的值\n",
    "    grad_h[h<0] = 0 #N*H；值大于0时grad就是1，所以保持原值\n",
    "    grad_w1 =X.T.dot(grad_h) #D_in*H <- D_in*H  N*H\n",
    "\n",
    "    #update weights of w1&w2\n",
    "    w1 -= learing_rate * grad_w1\n",
    "    w2 -= learing_rate * grad_w2\n",
    "\n",
    "    if i%5==0:\n",
    "        print(loss, grad_h_relu[0][0],grad_h[0][0],grad_w1[0][0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506 13 1\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n",
      "592.1469169960474 0.0 0.0 0.0 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9d61c7995a2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# Forword pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Compute loss（MSE）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#sum成标量\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-9d61c7995a2f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh_relu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# N*D_in D_in*H -> N*H\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mh_relu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# N*H...max函数是选某维度的max\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh_relu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# N*H H*D_out -> N*D_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "\n",
    "DATA = load_boston()\n",
    "\n",
    "# N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "# X = np.random.randn(N, D_in)\n",
    "# y = np.random.randn(N, D_out)\n",
    "\n",
    "X = np.array([[1,1],[2,1],[1,2],[2,2]])\n",
    "y = np.array([0,1,1,0,]).reshape(-1,1)\n",
    "X = DATA.data\n",
    "y = DATA.target.reshape(-1,1)\n",
    "# display(pd.DataFrame(X))\n",
    "# print(pd.DataFrame(X).info())\n",
    "\n",
    "X = Normalizer().fit_transform(X)\n",
    "# display(pd.DataFrame(X))\n",
    "N, D_in, D_out = X.shape[0], X.shape[1], y.shape[1] #N ：样本数，D_in：特征维度\n",
    "H = 500 #隐藏层神经元数\n",
    "print(N, D_in, D_out)\n",
    "w1 = np.zeros((D_in, H))\n",
    "\n",
    "w2 = np.zeros((H, D_out))\n",
    "# w1 = np.random.randn(D_in, H) \n",
    "# w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learing_rate = 1e-5\n",
    "EPOCH = 50000\n",
    "\n",
    "def forward(X):\n",
    "    global h,h_relu,y_pred\n",
    "    h = X.dot(w1)# N*D_in D_in*H -> N*H\n",
    "    h_relu = np.maximum(0, h)# N*H...max函数是选某维度的max\n",
    "    y_pred = h_relu.dot(w2)# N*H H*D_out -> N*D_out\n",
    "    \n",
    "\n",
    "for i in range(EPOCH):\n",
    "    # Forword pass\n",
    "    forward(X)\n",
    "    # Compute loss（MSE）\n",
    "    loss = np.square(y_pred - y).mean()#sum成标量\n",
    "\n",
    "    # Backward pass,compute gradient；即loss对参数求偏导，\n",
    "    # 因为是标量对向量/矩阵求，所以维度和向量一样就好。最后在纸上写出来\n",
    "    grad_y_pred = 2.0 * (y_pred - y) # N*D_out；因为是对y_pred求导，所以是y_pred-y\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)#H*D_out <- H*N N*D_out\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) #N*H <- N*D_out D_out*H\n",
    "    grad_h = grad_h_relu.copy() #浅拷贝，空间不同，但子对象相同。一般来说不能用赋值，否则可能改变被赋值对象的值\n",
    "    grad_h[h<0] = 0#N*H；值大于0时grad就是1，所以保持原值\n",
    "    grad_w1 =X.T.dot(grad_h) #D_in*H <- D_in*H  N*H\n",
    "\n",
    "    #update weights of w1&w2\n",
    "    w1 -= learing_rate * grad_w1\n",
    "    w2 -= learing_rate * grad_w2\n",
    "\n",
    "    if i%5==0:\n",
    "        print(loss,grad_h_relu[0][0],grad_h[0][0],grad_w1[0][0],w1[0][0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Tensor版，相较numpy改动之处：\n",
    "- np.random.randn -> torch.randn\n",
    "- .dot -> .mm\n",
    "- relu：np.maximum -> .clamp(min=0)\n",
    "    - Clamp all elements in input into the range [ min, max ] and return a resulting tensor:\n",
    "- loss\n",
    "- transpose：.t()\n",
    "- copy -> clone \n",
    "\n",
    "还可以放GPU上：\n",
    "- .cuda()相当于to.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from time import time\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "X = torch.randn(N, D_in).cuda() #放GPU上，本机更慢，可能是交互的原因；\n",
    "y = torch.randn(N, D_out).cuda()\n",
    "\n",
    "w1 = torch.randn(D_in, H).cuda() #w1 = np.zeros((D_in, H))\n",
    "w2 = torch.randn(H, D_out).cuda() #w2 = np.zeros((H, D_out))\n",
    "\n",
    "learing_rate = 1e-6\n",
    "EPOCH = 1000\n",
    "\n",
    "def forward(X):\n",
    "    global h,h_relu,y_pred\n",
    "    h = X.mm(w1)# N*D_in D_in*H -> N*H\n",
    "    h_relu = h.clamp(min=0)# N*H...max函数是选某维度的max\n",
    "    y_pred = h_relu.mm(w2)# N*H H*D_out -> N*D_out\n",
    "\n",
    "a = time()\n",
    "for i in range(EPOCH):\n",
    "    # Forword pass\n",
    "    forward(X)\n",
    "    # Compute loss（MSE）\n",
    "    loss = (y_pred - y).pow(2).sum().item()#item成标量\n",
    "\n",
    "    # Backward pass,compute gradient；即loss对参数求偏导，\n",
    "    # 因为是标量对向量/矩阵求，所以维度和向量一样就好。最后在纸上写出来\n",
    "    grad_y_pred = 2.0 * (y_pred - y) # N*D_out；因为是对y_pred求导，所以是y_pred-y\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)#H*D_out <- H*N N*D_out\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t()) #N*H <- N*D_out D_out*H\n",
    "    grad_h = grad_h_relu.clone() #深拷贝，空间不同，但子对象相同。一般来说不能用赋值，否则可能改变被赋值对象的值\n",
    "    grad_h[h<0] = 0#N*H；值大于0时grad就是1，所以保持原值\n",
    "    grad_w1 =X.t().mm(grad_h) #D_in*H <- D_in*H  N*H\n",
    "\n",
    "    #update weights of w1&w2\n",
    "    w1 -= learing_rate * grad_w1\n",
    "    w2 -= learing_rate * grad_w2\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(loss,grad_h_relu[0][0],grad_h[0][0],grad_w1[0][0])\n",
    "print('训练耗时：',time()-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(2.)\n",
      "tensor(1.)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "y = w*x+b # y = 2*1+3\n",
    "y.backward()\n",
    "\n",
    "print(w.grad) #x=1\n",
    "print(x.grad) #w=2\n",
    "print(b.grad) #1\n",
    "print(y.grad) #None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.PyTorch 来backward\n",
    "1. 先简化一波forword...\n",
    "+ backward\n",
    "    - loss不能变成item，还要保持tensor格式\n",
    "    - 把需要修改计算grad的tensor参数 加上requires_grad=True\n",
    "        - 变量默认不计算grad，节约内存\n",
    "    - w1.grad.zero_() 因为参数的grad默认是累加的\n",
    "\n",
    "+ with torch.no_grad() 不让计算图再保存一次grad，以节约内存\n",
    "    - 学习一下利用文档：其实讲的挺清楚的\n",
    "    - Context-manager that disabled gradient calculation.<br>Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). <br>It will reduce memory consumption for computations that would otherwise have requires_grad=True.<br>In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True.\n",
    "    - 目的 \n",
    "        1. 节约内存 \n",
    "        2. 能进行zeros_（因为requires_grad=True这样的叶子张量不允许inplace操作）\n",
    "+ 不能放cuda上？\n",
    "    - 原因：.cuda()只能把requires_grad=True的变量放到GPU上，而计算w需要用到xy的grad（None），这样就会得到None的grad\n",
    "    - 方法：device=device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from time import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "X = torch.randn(N, D_in, device=device) #放GPU上，本机更慢，可能是交互的原因；\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True, device=device) #w1 = np.zeros((D_in, H))\n",
    "w2 = torch.randn(H, D_out, requires_grad=True, device=device) #w2 = np.zeros((H, D_out))\n",
    "\n",
    "learing_rate = 1e-6\n",
    "EPOCH = 2000\n",
    "\n",
    "a = time()\n",
    "for i in range(EPOCH):\n",
    "    # Forword pass\n",
    "    y_pred = X.mm(w1).clamp(min=0).mm(w2)\n",
    "    # Compute loss（MSE）\n",
    "    loss = (y_pred - y).pow(2).sum()#保留tensor的type\n",
    "    # Backward pass,compute gradient；\n",
    "    loss.backward()\n",
    "    # update weights of w1&w2\n",
    "    with torch.no_grad():\n",
    "        w1 -= learing_rate * w1.grad\n",
    "        w2 -= learing_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(loss,w1.grad[0][0])\n",
    "print('训练耗时：',time()-a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.PyTorch封装模型\n",
    "改进：\n",
    "- model封装参数与forward\n",
    "- 参数初始化（提升性能）\n",
    "- loss_fn直接定义\n",
    "- update param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from time import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "X = torch.randn(N, D_in, device=device) #放GPU上，本机更慢，可能是交互的原因；\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H), #D_in -> H的线性变换。另：与之前不同的地方在于bias=True\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out),\n",
    ").to(device)\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learing_rate = 1e-6\n",
    "EPOCH = 5000\n",
    "\n",
    "a = time()\n",
    "for i in range(EPOCH):\n",
    "    # Forword pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute loss（MSE）\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # Backward pass,compute gradient；\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # update parameters\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters(): #model中的每个参数\n",
    "            param -= learing_rate * param.grad\n",
    "    \n",
    "    if i%1000==0:\n",
    "        print(loss, w1.grad[0][0])\n",
    "print('训练耗时：',time()-a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "Linear(in_features=1000, out_features=100, bias=True)\n",
      "torch.Size([100, 1000]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(7.7564e-05, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0183, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出模型\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H), #D_in -> H 的线性变换。另：与之前不同的地方在于bias=True\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out),\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(model[0])\n",
    "print(model[0].weight.shape, model[0].bias.shape)\n",
    "model[0].weight.mean(), model[0].weight.std() #发现这里随机初始化的weight都是非常小的数（应该是想近似0）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 进一步改进optimizer\n",
    "定义好优化器把grad清零和更新都搞定\n",
    "\n",
    "这里又发现：\n",
    "+ Adam，用正态分布ini反而很慢？？？t，不如直接不init（也就是上面看到的近似0的model中的init）\n",
    "+ SGD，用正态分布init更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(743.4058, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "117 0.0009817966492846608\n",
      "训练耗时： 0.42057013511657715\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from time import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "X = torch.randn(N, D_in, device=device) #放GPU上，本机更慢，可能是交互的原因；\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D_in, H, bias=True), #D_in -> H 的线性变换。另：与之前不同的地方在于默认bias=True\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out, bias=True),\n",
    ").to(device)\n",
    "# torch.nn.init.normal_(model[0].weight)\n",
    "# torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) #Adma一般设-3 -4，高级优化器一般刚开始调大一点，后面会自动优化\n",
    "EPOCH = 5000\n",
    "\n",
    "a = time()\n",
    "for i in range(EPOCH):\n",
    "    # Forword pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute loss（MSE）\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # Backward pass,compute gradient；\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    if loss<1e-3:\n",
    "        print(i, loss.item())\n",
    "        break\n",
    "    if i%100==0:\n",
    "        print(loss)\n",
    "\n",
    "print('训练耗时：',time()-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.自定义网络 继承nn.Module\n",
    "上面定义的model只是sequential模型，更复杂的就需要自己定义一个class，步骤：\n",
    "+ 定义class\n",
    "    1. architecture\n",
    "    2. forward\n",
    "+ model = Model(param)，后面不需要调用forward，直接y_pred = model(X)\n",
    "我手残，在self.linear1定义完打了逗号，这样python会把他和self.linear2看成一个tuple。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-e9b9f174f9db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'训练耗时：'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w1' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from time import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "X = torch.randn(N, D_in, device=device) #放GPU上，本机更慢，可能是交互的原因；\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        # define the model architecture\n",
    "        self.linear1 = nn.Linear(D_in, H, bias=True)\n",
    "        self.linear2 = nn.Linear(H, D_out, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = self.linear1(x).clamp(min=0)        \n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNN(D_in, H, D_out)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) #Adma一般设-3 -4，高级优化器一般刚开始调大一点，后面会自动优化\n",
    "EPOCH = 5000\n",
    "\n",
    "a = time()\n",
    "for i in range(EPOCH):\n",
    "    # Forword pass\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute loss（MSE）\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    # Backward pass,compute gradient；\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    if loss<1e-3:\n",
    "        print(i, loss.item())\n",
    "        break\n",
    "    if i%100==0:\n",
    "        print(loss, w1.grad[0][0])\n",
    "\n",
    "print('训练耗时：',time()-a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
