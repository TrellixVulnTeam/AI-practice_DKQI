{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二课 词向量\n",
    "\n",
    "第二课学习目标\n",
    "- 学习词向量的概念\n",
    "- 用Skip-thought模型训练词向量\n",
    "- 学习使用PyTorch dataset和dataloader\n",
    "- 学习定义PyTorch模型\n",
    "- 学习torch.nn中常见的Module\n",
    "    - Embedding\n",
    "- 学习常见的PyTorch operations\n",
    "    - bmm\n",
    "    - logsigmoid\n",
    "- 保存和读取PyTorch模型\n",
    "    \n",
    "\n",
    "第二课使用的训练数据可以从以下链接下载到。\n",
    "\n",
    "链接:https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg  密码:v2z5\n",
    "\n",
    "在这一份notebook中，我们会（尽可能）尝试复现论文[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)中训练词向量的方法. 我们会实现Skip-gram模型，并且使用论文中noice contrastive sampling的目标函数。\n",
    "\n",
    "这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。\n",
    "\n",
    "以下是一些我们没有实现的细节\n",
    "- subsampling：参考论文section 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  #神经网络工具箱torch.nn \n",
    "import torch.nn.functional as F  #神经网络函数torch.nn.functional\n",
    "import torch.utils.data as tud  #Pytorch读取训练集需要用到torch.utils.data类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**两个模块的区别：**[torch.nn 和 torch.functional 的区别](https://blog.csdn.net/hawkcici160/article/details/80140059)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter  #参数更新和优化函数\n",
    "\n",
    "from collections import Counter #Counter 计数器\n",
    "import numpy as np \n",
    "import random\n",
    "import math \n",
    "\n",
    "import pandas as pd\n",
    "import scipy #SciPy是基于NumPy开发的高级模块，它提供了许多数学算法和函数的实现\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity #余弦相似度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available() #有GPU可以用\n",
    "\n",
    "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(53113)\n",
    "    \n",
    "# 设定一些超参数   \n",
    "K = 10 # number of negative samples 负样本随机采样数量\n",
    "C = 3 # nearby words threshold 指定周围三个单词进行预测\n",
    "NUM_EPOCHS = 1 # The number of epochs of training 迭代轮数\n",
    "MAX_VOCAB_SIZE = 20000 # the vocabulary size 词汇表多大\n",
    "BATCH_SIZE = 64 # the batch size 每轮迭代1个batch的数量\n",
    "LEARNING_RATE = 0.2 # the initial learning rate #学习率\n",
    "EMBEDDING_SIZE = 100 #词向量维度\n",
    "       \n",
    "    \n",
    "LOG_FILE = \"word-embedding.log\"\n",
    "\n",
    "# tokenize函数，把一篇文本转化成一个个单词\n",
    "def word_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从文本文件中读取所有的文字，通过这些文本创建一个vocabulary\n",
    "- 由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词\n",
    "- 我们添加一个UNK单词表示所有不常见的单词\n",
    "- 我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"text8.test.txt\", \"r\", encoding='utf8') as fin: #读入文件\n",
    "#     text = fin.read()\n",
    "# text = [w for w in word_tokenize(text.lower())] \n",
    "# (text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"corpus_havestp_C.txt\", \"r\", encoding='utf8') as fin: #读入文件\n",
    "    text = fin.read()\n",
    "    \n",
    "text = [w for w in word_tokenize(text.lower())] \n",
    "#分词，在这里类似于text.split()\n",
    "\n",
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE-1))\n",
    "#字典格式，把（MAX_VOCAB_SIZE-1）个最频繁出现的单词取出来，-1是留给不常见的单词\n",
    "\n",
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
    "#unk表示不常见单词数=总单词数-常见单词数\n",
    "#这里计算的到vocab[\"<unk>\"]=29999\n",
    "\n",
    "idx_to_word = [word for word in vocab.keys()] \n",
    "#取出字典的所有单词key\n",
    "\n",
    "word_to_idx = {word:i for i, word in enumerate(idx_to_word)}\n",
    "#取出所有单词的单词和对应的索引，索引值与单词出现次数相反，最常见单词索引为0。\n",
    "\n",
    "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "#所有单词的频数values\n",
    "\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "#所有单词的频率\n",
    "\n",
    "word_freqs = word_freqs ** (3./4.)\n",
    "#论文里乘以3/4次方\n",
    "\n",
    "word_freqs = word_freqs / np.sum(word_freqs) # 用来做 negative sampling\n",
    "# 重新计算所有单词的频率\n",
    "\n",
    "VOCAB_SIZE = len(idx_to_word) #词汇表单词数30000=MAX_VOCAB_SIZE\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现Dataloader\n",
    "\n",
    "一个dataloader需要以下内容：\n",
    "\n",
    "- 把所有text编码成数字，然后用subsampling预处理这些文字。\n",
    "- 保存vocabulary，单词count，normalized word frequency\n",
    "- 每个iteration sample一个中心词\n",
    "- 根据当前的中心词返回context单词\n",
    "- 根据中心词sample一些negative单词\n",
    "- 返回单词的counts\n",
    "\n",
    "这里有一个好的tutorial介绍如何使用[PyTorch dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "为了使用dataloader，我们需要定义以下两个function:\n",
    "\n",
    "- ```__len__``` function需要返回整个数据集中有多少个item\n",
    "- ```__get__``` 根据给定的index返回一个item\n",
    "\n",
    "有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(tud.Dataset): #tud.Dataset父类\n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\n",
    "        ''' text: a list of words, all text from the training dataset\n",
    "            word_to_idx: the dictionary from word to idx\n",
    "            idx_to_word: idx to word mapping\n",
    "            word_freq: the frequency of each word\n",
    "            word_counts: the word counts\n",
    "        '''\n",
    "        super(WordEmbeddingDataset, self).__init__() #初始化模型\n",
    "        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE-1) for t in text]\n",
    "        #字典 get() 函数返回指定键的值（第一个参数），如果值不在字典中返回默认值（第二个参数）。\n",
    "        #取出text里每个单词word_to_idx字典里对应的索引,不在字典里返回\"<unk>\"的索引\n",
    "        #\"<unk>\"的索引=29999，get括号里第二个参数应该写word_to_idx[\"<unk>\"]，不应该写VOCAB_SIZE-1，虽然数值一样。\n",
    "        \n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "        #变成tensor类型，这里变成longtensor，也可以torch.LongTensor(self.text_encoded)\n",
    "        \n",
    "        self.word_to_idx = word_to_idx #保存数据\n",
    "        self.idx_to_word = idx_to_word  #保存数据\n",
    "        self.word_freqs = torch.Tensor(word_freqs) #保存数据\n",
    "        self.word_counts = torch.Tensor(word_counts) #保存数据\n",
    "        \n",
    "    def __len__(self): #数据集有多少个item \n",
    "        #魔法函数__len__\n",
    "        ''' 返回整个数据集（所有单词）的长度\n",
    "        '''\n",
    "        return len(self.text_encoded) #所有单词的总数\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        #魔法函数__getitem__，这个函数跟普通函数不一样\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的(positive)单词\n",
    "            - 随机采样的K个单词作为negative sample\n",
    "        '''\n",
    "        center_word = self.text_encoded[idx] \n",
    "        #print(center_word)\n",
    "        #中心词索引\n",
    "        #这里__getitem__函数是个迭代器，idx代表了所有的单词索引。\n",
    "        \n",
    "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))\n",
    "        #周围词索引的索引，比如idx=0时。pos_indices = [-3, -2, -1, 1, 2, 3] \n",
    "        #老师讲这里的时候，我不是特别明白\n",
    "        \n",
    "        pos_indices = [i%len(self.text_encoded) for i in pos_indices]\n",
    "        #range(idx+1, idx+C+1)超出词汇总数时，需要特别处理，取余数\n",
    "        \n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        #周围词索引，就是希望出现的正例单词\n",
    "        #print(pos_words)\n",
    "        \n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
    "        #负例采样单词索引，torch.multinomial作用是对self.word_freqs做K * pos_words.shape[0]次取值，输出的是self.word_freqs对应的下标。\n",
    "        #取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大。\n",
    "        #每个正确的单词采样K个，pos_words.shape[0]是正确单词数量\n",
    "        #print(neg_words)\n",
    "        \n",
    "        return center_word, pos_words, neg_words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建dataset和dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "# list(dataset) 可以把尝试打印下center_word, pos_words, neg_words看看\n",
    "\n",
    "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)     \n",
    "# next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.data.DataLoader理解：https://blog.csdn.net/qq_36653505/article/details/83351808"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义PyTorch模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        ''' 初始化输出和输出embedding\n",
    "        '''\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.vocab_size = vocab_size  #30000\n",
    "        self.embed_size = embed_size  #100\n",
    "        \n",
    "        initrange = 0.5 / self.embed_size\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        #模型输出nn.Embedding(30000, 100)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        #权重初始化的一种方法\n",
    "        \n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "         #模型输入nn.Embedding(30000, 100)\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        #权重初始化的一种方法\n",
    "        \n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        input_labels: 中心词, [batch_size]\n",
    "        pos_labels: 中心词周围 context window 出现过的单词 [batch_size * (window_size * 2)]\n",
    "        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (window_size * 2 * K)]\n",
    "        \n",
    "        return: loss, [batch_size]\n",
    "        '''\n",
    "        \n",
    "        batch_size = input_labels.size(0)  #input_labels是输入的标签，tud.DataLoader()返回的。相已经被分成batch了。\n",
    "        \n",
    "        input_embedding = self.in_embed(input_labels) \n",
    "        # B * embed_size\n",
    "        #这里估计进行了运算：（128,30000）*（30000,100）= 128(B) * 100 (embed_size)\n",
    "        \n",
    "        pos_embedding = self.out_embed(pos_labels) # B * (2*C) * embed_size\n",
    "        #同上，增加了维度(2*C)，表示一个batch有B组周围词单词，一组周围词有(2*C)个单词，每个单词有embed_size个维度。\n",
    "        \n",
    "        neg_embedding = self.out_embed(neg_labels) # B * (2*C * K) * embed_size\n",
    "        #同上，增加了维度(2*C*K)\n",
    "      \n",
    "    \n",
    "        #torch.bmm()为batch间的矩阵相乘（b,n.m)*(b,m,p)=(b,n,p)\n",
    "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)).squeeze() # B * (2*C)\n",
    "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze() # B * (2*C*K)\n",
    "        #unsqueeze(2)指定位置升维，.squeeze()压缩维度。\n",
    "        \n",
    "        #下面loss计算就是论文里的公式\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1) # batch_size     \n",
    "        loss = log_pos + log_neg\n",
    "        \n",
    "        return -loss\n",
    "    \n",
    "    def input_embeddings(self):   #取出self.in_embed数据参数\n",
    "        return self.in_embed.weight.data.cpu().numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个模型以及把模型移动到GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "#得到model，有参数，有loss，可以优化了\n",
    "\n",
    "USE_CUDA = 0\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "下面是评估模型的代码，以及训练模型的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(filename, embedding_weights): \n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename, sep=\",\")\n",
    "    else:\n",
    "        data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    human_similarity = []\n",
    "    model_similarity = []\n",
    "    for i in data.iloc[:, 0:2].index:\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
    "            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
    "            human_similarity.append(float(data.iloc[i, 2]))\n",
    "\n",
    "    return scipy.stats.spearmanr(human_similarity, model_similarity)# , model_similarity\n",
    "\n",
    "def find_nearest(word):\n",
    "    index = word_to_idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型：\n",
    "- 模型一般需要训练若干个epoch\n",
    "- 每个epoch我们都把所有的数据分成若干个batch\n",
    "- 把每个batch的输入和输出都包装成cuda tensor\n",
    "- forward pass，通过输入的句子预测每个单词的下一个单词\n",
    "- 用模型的预测和正确的下一个单词计算cross entropy loss\n",
    "- 清空模型当前gradient\n",
    "- backward pass\n",
    "- 更新模型参数\n",
    "- 每隔一定的iteration输出模型在当前iteration的loss，以及在验证数据集上做模型的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19999, 19999,  2035,  2935, 19999,    49, 19999,   799, 10964, 19999,\n",
      "           65,    10,    58,   308, 19999, 19999, 19999, 19999,  1535, 19999,\n",
      "        19999, 12049,   712,  5072,   280,  9481,  1035,   196,  1855,   204,\n",
      "           55,   294,   171,  1188,    32, 19999,  5829,   842,    20, 19999,\n",
      "          590,  3357,    11,  1046,   775,     5, 19999,  7946,  3762,  4181,\n",
      "           10,  3569,    32,  1533, 19999,  5286,  5549,   154, 19999,   930,\n",
      "          368,   547,   138,     6]) tensor([[15234,  1030,  6248,  3003,  3800,  7867],\n",
      "        [11756, 19999, 17770, 19999, 19999, 19999],\n",
      "        [  586,  9116,  1245,    91,    10, 10663],\n",
      "        [ 2935,   116,  3325,   135,   227,    26],\n",
      "        [19999,   661,  1916, 11250, 19999,  5804],\n",
      "        [19999,  3237,  2572,    18, 19999,   167],\n",
      "        [ 1445, 19999, 19999, 16860,   187, 19999],\n",
      "        [19999, 19999,    18,  5327, 14668, 19999],\n",
      "        [  473,   354,    47, 12238, 19999,    56],\n",
      "        [19999, 19999, 19999, 19999, 19999, 19999],\n",
      "        [ 4970,  2678,   653,  6009, 19999,  3341],\n",
      "        [ 1033,    26,   150,  2185, 19999,   594],\n",
      "        [19999, 19999,   788,    47,   123,   214],\n",
      "        [    1,   737,  1172,   215,  1166,   215],\n",
      "        [ 1697,  4857,  1592, 19999,  5421,  9542],\n",
      "        [ 1490,  7315, 19999,  1322,   385,  4257],\n",
      "        [  104,    17,  1345, 19999, 19999, 19999],\n",
      "        [19999, 19999,  1443,  1852,  1443,   301],\n",
      "        [  275, 19999, 19999,  3232,  4379,  1334],\n",
      "        [ 2876, 19999,  2269,    43, 19999,   176],\n",
      "        [ 9734, 19999,  9734,  4433, 19999,  1912],\n",
      "        [ 2628, 19999,  1104,   261,  2070,    83],\n",
      "        [ 2143,   876, 15158,  3309,   745,  1554],\n",
      "        [  134,   678,  1006,    14,   192, 19999],\n",
      "        [  145,   280,   236,   236,   280,     3],\n",
      "        [   70,  6350,  3644,    70,    69,  2729],\n",
      "        [ 3793, 19999,    98, 19999,  6834,  6920],\n",
      "        [ 2021, 12829, 16336, 19999, 19999, 19999],\n",
      "        [ 2192,  2774,   584,   610,  1035,  1027],\n",
      "        [  150, 12210,   174,   309,    36, 19999],\n",
      "        [    7,   257,   309,  2719,  5150,    66],\n",
      "        [  157,  9274,  5755,   478,  5755,  9343],\n",
      "        [14200,   171,   889,   305, 14130,   484],\n",
      "        [  962,   268,     7,  6444,   678,  4982],\n",
      "        [13126, 19999, 19999,  8149,   718,  1136],\n",
      "        [   53,   270,  6396,   429,     6,    69],\n",
      "        [ 7391,  1385, 19999, 19999,  1385, 19999],\n",
      "        [16990, 19999, 19999, 19999,  6030, 19999],\n",
      "        [  356,  2458,   660, 10947, 19999,  1956],\n",
      "        [19572,  1147,   231,  7392,  1147, 11692],\n",
      "        [  423,   165, 12669,  2883,  1305,   111],\n",
      "        [    8,    50, 19999,   179,   780,  1023],\n",
      "        [ 2458,   201,  2262,  9943,  2260,    38],\n",
      "        [19999, 12910, 11990,  6471,   229,  4382],\n",
      "        [19999,  1068, 16247,   965,   532,  5106],\n",
      "        [19999, 19999,     0,   293, 10169,  1074],\n",
      "        [11922,     3,  4776,   719,  2062,    48],\n",
      "        [   12,   126,     0,    72,  2764,    46],\n",
      "        [19999,   160,  2580,  8551, 19999, 19999],\n",
      "        [ 4181,  9151, 19999, 12495,  2390, 19863],\n",
      "        [  247,   116,     0, 19999,  6837,   506],\n",
      "        [19999,  6125, 19999,  3908, 15944,   489],\n",
      "        [   28,  1663,  3470,   804, 16155, 19999],\n",
      "        [19999, 19999,  4101,   496,    40,   628],\n",
      "        [ 3468, 19999,   825,  4923,   391,  1604],\n",
      "        [19999,  5662,  8831,   202,  1741,  1252],\n",
      "        [ 5151, 12950,  3217, 19999,  5658, 12635],\n",
      "        [ 6077,  3207,    63,  4974, 10325,   154],\n",
      "        [19999,  2779,    21, 10691,   436,    18],\n",
      "        [19999,  4356,  1892,   329,  1318, 19999],\n",
      "        [  665,  3161,    46,  3334,    72,  7453],\n",
      "        [19999,  4747, 19999, 19999,   649,  1984],\n",
      "        [ 7885,  8525,  1655,  1977,  1107,  5795],\n",
      "        [  216,  1352, 19999, 19999, 19999,     6]]) tensor([[  277,   378,   791,  ..., 15762,   190,  3579],\n",
      "        [ 4405,   846,   747,  ...,  1284,   520,   138],\n",
      "        [18630,  2610,  4470,  ...,  2439, 13536,  4287],\n",
      "        ...,\n",
      "        [  757,  6972,  1076,  ...,  3914,  2980,   389],\n",
      "        [10080,  3096, 12155,  ..., 18031,  8730,    23],\n",
      "        [ 8162,  3537,  8601,  ...,  4214,   206, 11719]])\n",
      "tensor([ 1373,  7588, 19999,  5963,     1, 19999, 19999, 19999,    38,  3885,\n",
      "         1198, 19999,  4367,  1493,  1324,    54,  1855, 19999, 19999, 10363,\n",
      "         5957,  7294,   379,  5661,  1439,    42,  9807,  6382,   254,   400,\n",
      "          350, 13537,  1089,    16,  1077,  4399,  2073,   107,  6930,  3000,\n",
      "          483, 19999,   502,  4170,  6870, 16586,  6091, 19999,  2999,  6231,\n",
      "        10013,  4261, 13680,   174, 19999,   844,   343,  1451,  1452,  1588,\n",
      "        19999,   130,  2635,  2616]) tensor([[  593,   816, 19999,    19,  2116, 19999],\n",
      "        [19999, 19999,   891,  2777,  1003,  2241],\n",
      "        [19281, 19999,    49,  3463,  4517,  4045],\n",
      "        [19999,  2985,     4, 19999, 19999,  6319],\n",
      "        [ 1629,   189, 19999,     1,  6745, 19999],\n",
      "        [  625, 19999, 19999, 19279, 19999, 19563],\n",
      "        [19999,  8054,   952,    43,  2148, 19999],\n",
      "        [  136, 13877,  5475,  3764,  3989,  1754],\n",
      "        [19999,  1010,  8717,   300,  1468,   130],\n",
      "        [ 9778,  6919,   327,  1601, 19999, 19999],\n",
      "        [10206,  2337,    60, 19999,  7820, 19999],\n",
      "        [ 5646,   292,  1052,  9859, 19999, 19999],\n",
      "        [   95,   105, 12077,   592,   241,   523],\n",
      "        [  149,   198,  4798,  1440,  1115,  1422],\n",
      "        [  164,   481,  6461,  3232,  1619,  7117],\n",
      "        [19999, 19999, 19999,   372,  9325,  3065],\n",
      "        [   66,    78,  4117,  1228,  2009,   251],\n",
      "        [  633,   194, 19999, 19999,  4917,   575],\n",
      "        [ 2730,  4333,  4333, 19999,  5359,  5999],\n",
      "        [19999, 19999, 19999, 19999, 19999, 19999],\n",
      "        [  902,  6763,   902,   303,  3005,   327],\n",
      "        [   29,  9110,    24,  7940,   494,  2222],\n",
      "        [19999,   573,    93,    18, 19999,    17],\n",
      "        [   78,    29,  3577, 19999,  2864,   867],\n",
      "        [   22,  6017,  5074,    37,   586, 18111],\n",
      "        [ 1232,    21,    44,   148,    63,    44],\n",
      "        [   23,   682,    88,    23,   157, 19999],\n",
      "        [   16,  1290,  1765, 19999,   409,  4248],\n",
      "        [   11,    11,  3551,  7786,  1736, 13411],\n",
      "        [   97,   226,    91, 19999,    20,  5719],\n",
      "        [   66,    78, 19999,  3019,    66,    78],\n",
      "        [19999,   623,  1410,    70, 19999,  1610],\n",
      "        [ 3078,  7152,   195,   745,   195,   780],\n",
      "        [19999, 19999, 19999, 19999, 19999, 19999],\n",
      "        [  141,   788,   875,    46,     2, 12021],\n",
      "        [ 9348, 19999,  2315,   174,  1034,     2],\n",
      "        [19999,   901, 10738, 19999, 11892,  2026],\n",
      "        [  119,  7369,  7893,  1102, 11321, 16596],\n",
      "        [  452, 19999,  4568,  2621,  4691,  9841],\n",
      "        [19999,   193,  7387,  2040,    26,  1681],\n",
      "        [ 3344,  1417,   843,  6483, 19999,  1741],\n",
      "        [ 1433, 19999, 19999, 11257, 15384,  3164],\n",
      "        [10645,  6192,   166,  6076,     3, 19999],\n",
      "        [  135,   227,   278, 19999,  4238, 19999],\n",
      "        [  769,  6870, 17428,  8318, 19999,  1086],\n",
      "        [ 4729,  4729,  1537,     2, 19999, 19999],\n",
      "        [ 3971,   120,   956,    25,     3,  6806],\n",
      "        [ 2106,   291,     1, 19999, 19999,     7],\n",
      "        [ 2760,  1484,   833, 16221, 19999,  6731],\n",
      "        [  635,   455,   147,    11,  2418, 19999],\n",
      "        [19999,  3849, 19999,   988, 19999,    88],\n",
      "        [ 2902, 19999,  1141,  3517,   132, 19999],\n",
      "        [ 1261,  1396, 19999,  3946, 19999, 19999],\n",
      "        [19999,   226, 14181,   426,   578,   972],\n",
      "        [  111,  2136,  4950,  8126, 19999, 15271],\n",
      "        [  494,   826,     5,  1260, 19999, 19999],\n",
      "        [ 2209,  1865,  7820,  1053,  3591, 19999],\n",
      "        [ 3301, 19999,  2146,  8948, 19999, 19999],\n",
      "        [  126,  1158,   548, 19409,  1156,     2],\n",
      "        [ 5867,  1741,  1496, 19999,  1723, 15709],\n",
      "        [19999, 16503, 19999, 19999, 19999, 13712],\n",
      "        [12310, 19999,  5148,  3373,  4628,   160],\n",
      "        [19999,   126, 19999,   166,  3220,  6986],\n",
      "        [   78,    38,  3666,     0,     5,  1532]]) tensor([[16718,  9089,   727,  ...,   499,   467,  4731],\n",
      "        [ 9207,  8180,   411,  ...,    63,  1802,  1675],\n",
      "        [ 2716,  9952, 13765,  ...,  4381, 19999,  8826],\n",
      "        ...,\n",
      "        [ 9782,   792,  1893,  ...,  4831, 13290,  9729],\n",
      "        [  122, 18258,  6980,  ...,  6886,  8139, 19999],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [10875,  4226,   506,  ...,   298,   605,   581]])\n",
      "tensor([  129,    16,   244, 19999,   392, 19999,   653, 19999,  1349,  2141,\n",
      "          244,   727,  8009,  2189,   215, 19999,   457,  1908, 19999,  2902,\n",
      "           46, 19999, 19999, 14918,   264, 19999, 12723,   365, 14880,  4177,\n",
      "          888,  2940, 10329,   144,  3097, 19999, 19999,   126,  1355, 19999,\n",
      "         5785,   687, 18725,  5697,  6292, 17160,   321,   133, 19999,  1543,\n",
      "          501,   251, 19999,  3087,  1441,    20,  4224,  3126,  6801,  4274,\n",
      "         3458,   168, 11779, 19999]) tensor([[ 5243,  5575,   427,  2334,  7036, 19064],\n",
      "        [ 3077, 19999,   762,  6603,  4386, 19999],\n",
      "        [ 3763,  2594, 12273,  7805, 19999,  1204],\n",
      "        [  166,  1629,   189,     1,     0,     1],\n",
      "        [19999,   523,   724, 16900, 19999,   523],\n",
      "        [  303, 11777,  1245, 19999,  1128, 11350],\n",
      "        [   63,  4888,   576,   265,   615,  6612],\n",
      "        [19999,  1824, 19999, 19999, 11680,  7049],\n",
      "        [   40,  1008,  1134,  1382,    95,  1276],\n",
      "        [10081,  2156,  1547, 10093,   323, 18816],\n",
      "        [17373,  4477,   139,  1365,    71,   255],\n",
      "        [  116,   697,   111,  2236,  1408,  5317],\n",
      "        [   67,    66, 19999, 19999,  1276,  1413],\n",
      "        [19999, 19999, 12158,  8828,  5984,   287],\n",
      "        [  531,   647,  2179,   288,    56, 19999],\n",
      "        [ 7108,   853,   764, 19999, 19999,  1709],\n",
      "        [   87,  1436,   452, 10856,  1442,  1448],\n",
      "        [19999,  9263,   224,  5440,  1729,   391],\n",
      "        [  899,  3111, 14222,  2122,  8832,  2122],\n",
      "        [  153,    51,   711, 11295,   110,   535],\n",
      "        [18786,  4799,  1423,  3727, 14732,  6211],\n",
      "        [ 1708, 19999, 19999,  4057,   180,   593],\n",
      "        [   79,  2603, 19999,  1950, 19999,  2107],\n",
      "        [ 4243,  4159,  3084,   124,  9272, 19999],\n",
      "        [  712,  6215,  1027,  9960,  5345,  1313],\n",
      "        [19999, 19999, 19999, 19999,  3533,   548],\n",
      "        [ 3016,  1881,  3669,  8169,  2213,    51],\n",
      "        [19999, 19999, 19999, 19326, 19999, 19999],\n",
      "        [  675,   561, 10813, 15180, 12093, 14455],\n",
      "        [10225,    34,   593, 19999,   256,  1707],\n",
      "        [19761,  6278,     7,  2010,  1582,   305],\n",
      "        [  267,    55,  8876, 11252, 17561,   611],\n",
      "        [  245,   330,   674,  7723, 19999,  9583],\n",
      "        [19999, 11013,   252,  1080,  1553, 19999],\n",
      "        [ 1142,  2212,  4408, 19999, 19999, 19999],\n",
      "        [ 6383, 19999,  1450, 19999, 11257,   161],\n",
      "        [ 4039,  1145,     2,  2676,  1462, 19999],\n",
      "        [   16,   183, 11450,   347,  1748,  7636],\n",
      "        [   61,  1500, 19999, 19999,  2074, 19999],\n",
      "        [ 2761,  7072,     2,  1265,  1417,  5046],\n",
      "        [18588,   545,   453, 19999, 18589, 16647],\n",
      "        [19999,  3793, 19999,   103,   267,   645],\n",
      "        [  521,  5390,  3648, 10514,    26,    17],\n",
      "        [  474,  5127,  1154,   289,  8358,     1],\n",
      "        [10600, 10779, 19999, 19999, 19999, 19999],\n",
      "        [ 6698, 19999, 10302, 17466, 19999,    39],\n",
      "        [  592,   967,    80,  9854,  8587,  1381],\n",
      "        [ 4701,  1724,   272,    68,   239, 14458],\n",
      "        [12012,  4839,  9793,  5579,  8799,   103],\n",
      "        [19658,  2627,  1506,  2748, 19999, 15137],\n",
      "        [  256,   329, 19999,  1619,  7273,  3821],\n",
      "        [  797,  3007,   881,   681, 10546, 19999],\n",
      "        [ 2315, 19826,     5,   105,   941,    87],\n",
      "        [ 1481,  1023,   454,    51,    64, 19999],\n",
      "        [   39,  1770, 19999,   926,  1745,    23],\n",
      "        [  766,  5440,   340,   667,  4740,  4882],\n",
      "        [ 5791,  2998,  3887,   166,  3123,  1546],\n",
      "        [  185, 19999,  1137,  3066,   155,  2896],\n",
      "        [ 2665, 19999, 19520,  4813,   404,   368],\n",
      "        [  117, 19999,  3733,  1467, 19999, 19496],\n",
      "        [ 4264,  1541,   332,    47,  2494,   298],\n",
      "        [   13,  1382,  1508, 15239,  8343, 13574],\n",
      "        [19999, 17887,   464, 19999, 19999,  5623],\n",
      "        [ 1700, 19628,    26, 12966, 11734, 19999]]) tensor([[   22,   145,  2575,  ..., 18949, 19921,  1894],\n",
      "        [ 1940,  1094,   186,  ..., 13488,  5024,    27],\n",
      "        [  231,    37,  9576,  ...,  1425, 19841,  4754],\n",
      "        ...,\n",
      "        [ 5286, 18738,  3911,  ...,  1393,  4498, 15337],\n",
      "        [ 7566,    56,     3,  ..., 19999, 11330,   891],\n",
      "        [    3,  7952,  1212,  ...,  1438,  8400,  1687]])\n",
      "tensor([ 4893,  7048,  3028,  3360,   800,  4983,   195,   100, 19999, 16730,\n",
      "         3910,   778, 19999, 19999,    32,     0,    93, 10443,   533,   631,\n",
      "        19999,  1800, 19999, 16920,  3413,  4859,    17,  1238,  1740,  6905,\n",
      "         2255,     6,  5670,  6902, 19999,   772,  3242, 19273,     6,  2406,\n",
      "        19999,  1114,  8918,  2271, 19999, 12606, 19999, 14925, 17552, 15921,\n",
      "        19999, 19999,  2841,  1006,   223,  1580,  1268,  2068, 19999,   952,\n",
      "          440,    27,    13,   593]) tensor([[19999,   787,   973, 19999,  1044,   241],\n",
      "        [ 1319, 19999,  1083, 16894,   223,   160],\n",
      "        [ 5072,  3115,  2110,  1066,    22,     1],\n",
      "        [  575,  1180,  1066,  2609,     3,   485],\n",
      "        [  357,    25,  1596,   357,    25,    86],\n",
      "        [ 3829,  2141,    89, 19999, 13912, 19999],\n",
      "        [17379,   806,    51,   797,   686,    23],\n",
      "        [ 5437,   162,  1202,  1249, 19999,  9981],\n",
      "        [   70,  9286,  1479,   174,   151,    43],\n",
      "        [19999,  9422,  1323,    57,  5635,  3551],\n",
      "        [19999, 10765, 19999,  7124, 19999, 19999],\n",
      "        [19999,  1183,  8923, 19999, 19999,  3975],\n",
      "        [ 4030,   280,    23,   120,  1048, 19999],\n",
      "        [ 2281, 19999, 19999, 18335, 19999, 15165],\n",
      "        [ 1378,  5932,    54,   812, 19942,    54],\n",
      "        [  233,  7344, 19999,  1346, 19999,   840],\n",
      "        [  757,   783,   527,   320,  8857,  1834],\n",
      "        [19999,  1660, 12636,  8324,  8419, 19999],\n",
      "        [ 2088, 19999,   139,  2601,  2927,  4580],\n",
      "        [   37,  1389,   642,    72,  6641,   647],\n",
      "        [ 3596,    29,    41, 15757,   642,   673],\n",
      "        [   74,   879,  5892,    59,  9822,   317],\n",
      "        [  442,  3151,  6035,  1032, 19999,  1690],\n",
      "        [ 9574, 19999, 19999,  3389,  1295, 19999],\n",
      "        [  125, 19999, 19999,  3701, 19999,  3352],\n",
      "        [ 5587, 19999,   446,   741,  2193,  2209],\n",
      "        [  973, 19999,    12,  1631,  1117, 19999],\n",
      "        [14793,  7774,  3897,  2331, 19999,  1013],\n",
      "        [ 4113, 11443,    28, 19999, 19999, 19999],\n",
      "        [19999,  3678,  5013,  4739, 19999, 19999],\n",
      "        [  250,   229,   877,   581,  5184,  5600],\n",
      "        [ 8896, 19999, 19999, 19999, 10639,  1112],\n",
      "        [ 3116,  5407,  2626,   428,  2188,   131],\n",
      "        [   71, 19999, 19365,    76,  6449, 19999],\n",
      "        [ 3377, 19999,  1036,    88, 11217,   464],\n",
      "        [  325,  2351,  2942, 19999,    69,   477],\n",
      "        [ 7241,  7241,   188,   162, 19999,  2289],\n",
      "        [19999, 16047, 13725, 19999, 19999,  1429],\n",
      "        [ 4099,  1044,  2831,     4,   739,  4099],\n",
      "        [19999,  2406, 19999,  2604, 19999,  1092],\n",
      "        [ 4217,    87,    25,   402,  2637,   296],\n",
      "        [ 9387,   662,   594,  3685,  8919,   109],\n",
      "        [  931,  3078, 12913, 17512, 19999,   931],\n",
      "        [  759, 19999,   334,   541,    94,   224],\n",
      "        [   38,  3502,    38, 19999,    38,  6874],\n",
      "        [ 3458,     2,   541,  1311,   264, 14505],\n",
      "        [  576, 19999,   209,  1580,  1418,    99],\n",
      "        [19999,    68,  3552,     1,  2965, 11676],\n",
      "        [19999,  2547, 19999,   325, 19999,   634],\n",
      "        [  400,     6,   559,  1794,  2629,   313],\n",
      "        [    0,     5,  2722,    57,     6,  7884],\n",
      "        [11891, 10374,   524,  3414,  8442, 19999],\n",
      "        [  276,  2718,  3158,   968,  2825,  4165],\n",
      "        [10051,  3298,   678,  6909,     3,  4408],\n",
      "        [15920,   351,   496,   271,  5297,    13],\n",
      "        [ 1344, 12647,   563, 11209,   103,  1846],\n",
      "        [  287,   327,   223,    62,   668,    10],\n",
      "        [  233,   114,     6,  2479, 13448,   553],\n",
      "        [ 1924,  7290,  1402, 19999,  7444, 19999],\n",
      "        [19999, 18148,  1622, 19999,  3169,   244],\n",
      "        [  234,   871,   109,   278,  2646,   944],\n",
      "        [  405,   724,   122,   122, 11441,   122],\n",
      "        [    6, 19999,  2966,     6, 10921,  7204],\n",
      "        [   69,     7,  7692, 11898,  2525,  1863]]) tensor([[ 1944, 16139,  2331,  ...,  1461,  9815,  7960],\n",
      "        [10772,  3968,  1716,  ...,   247,   586,  3428],\n",
      "        [ 1358,  3021,    69,  ...,  1445,  1153,  7778],\n",
      "        ...,\n",
      "        [ 6287,  1051, 15063,  ...,  8102, 14432,   126],\n",
      "        [  590,  4061,    48,  ...,   208,  4818, 14670],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 4821,  1389,    85,  ..., 15156,  2830,   123]])\n",
      "tensor([  408, 12225,  2485, 19999,  7588, 19999,   174,  2150,   206,  1954,\n",
      "           44,  6066,  9765,  1017,   475,  1476,  1177,     5,   319,    83,\n",
      "         8575, 19999,   142,    88,   739,  2409,   274, 19999,  6885,   146,\n",
      "          904,   162,  1707,  3045,  3999,  3161,  3806, 19999,   264,  1726,\n",
      "         2185, 11659,    22, 19999,   405, 18232,  1987,  2428,   591,  4046,\n",
      "          374,  6927,  2891,  3315,   212,   355,   411,  4024,    19, 19999,\n",
      "         1180,   306,  4172,  1823]) tensor([[15826, 13549,   265,  2046,  1467,  6852],\n",
      "        [13151,    33, 12548,  1011,  3290,  3415],\n",
      "        [ 9105, 19999,  3571,  3807, 19999, 19999],\n",
      "        [14874, 15890,  1513, 14874,  1691, 14874],\n",
      "        [ 1116, 15365, 13671,   364, 19891, 19999],\n",
      "        [17285,   155,  1129,   516,   290,  2975],\n",
      "        [ 9133, 19999, 19999,  5732, 19999,  8523],\n",
      "        [ 2340,  2113,  7550,  2424, 19999,   958],\n",
      "        [   35, 16241,   264,  8568,  3210,  7762],\n",
      "        [19999, 19739,   123,   362,    84,     2],\n",
      "        [19999, 19999,   180,   755,  1813, 19999],\n",
      "        [ 1742,   337,  1469, 12950, 18125,   406],\n",
      "        [   48,   885, 15745, 15983,  2579,  1366],\n",
      "        [ 6603,  8412, 19999,    15, 19999,  1768],\n",
      "        [  447,   994, 19999,   696,  2298,   155],\n",
      "        [13238,  5040, 10966, 19999, 19999, 19999],\n",
      "        [  348,  5052,  3340, 16912,  3466,  9310],\n",
      "        [  388,    62,    15,  7046,   855,  1298],\n",
      "        [ 1784,  7475,  5993,   581, 19999,   182],\n",
      "        [ 2346,  1582,  8249,  3910,  4417,   257],\n",
      "        [16369,   182, 19999, 19999,  1002, 17506],\n",
      "        [   82,    82,   598,   141,   568, 19999],\n",
      "        [  169,  2983,  5082,  7816,   242,  2552],\n",
      "        [  268,  4828, 19999, 15494,    99,  3515],\n",
      "        [  452, 19999,   862, 19999,  5412,  1660],\n",
      "        [ 5067,     5, 19999,  1034,  3700,  6260],\n",
      "        [  128,   796,  1173,  3467,   184,   954],\n",
      "        [  553, 19999,  3268,   283,  3591,  1163],\n",
      "        [19977, 19999,  1191, 19999,  9080,   883],\n",
      "        [19999, 13440,    76,  1015, 19999,   229],\n",
      "        [ 1731,  2824,   110,  4077,   382,  4689],\n",
      "        [ 2381,  3051,   203,  5871,   339,   286],\n",
      "        [   22,    94,   940,   793,   997,    13],\n",
      "        [    5,   229, 16291, 10939,  1589,   126],\n",
      "        [19999,  1473,    42,   229,   786,   226],\n",
      "        [19999, 19548,     7, 19999, 16203, 18910],\n",
      "        [ 6405,  3126,  3423,   378,   229,   817],\n",
      "        [11245,  6533,  2039,  2039, 14093,  9090],\n",
      "        [17549, 19999,  8640,  1569,  1888,  2325],\n",
      "        [  393, 19999, 19999,  2320,   928,   776],\n",
      "        [ 1799, 18456,     9, 17545,  9324,  5127],\n",
      "        [ 2606,  4229,  2362, 19999, 19999, 19999],\n",
      "        [ 7983, 19788,   805, 14195, 19999, 19999],\n",
      "        [ 5233,   352, 19999,  1817,  5721, 19999],\n",
      "        [  141, 19999,  6181,  2406, 17156,    35],\n",
      "        [ 7903,    16,   808,   204,   651,    36],\n",
      "        [ 8706,    85,  1358, 11041,    12,   300],\n",
      "        [ 2082,   200,   238,   295,    58,  9967],\n",
      "        [  219,    38,   423,  3448,   423, 18888],\n",
      "        [19999, 19999, 10246,  6447,  6990,  4683],\n",
      "        [  541,  7833, 19999,  5382,   181,  3503],\n",
      "        [ 8539, 11383,   830,  5665,    23, 18218],\n",
      "        [   22, 19999, 11184,     0,     5,  5959],\n",
      "        [ 7035,  1643,   484,   312,   312, 10094],\n",
      "        [ 5894, 19999,  5773,  1253,  1110,     8],\n",
      "        [  297,   102,  4530,  6563,     2,   273],\n",
      "        [15044,    13,   178,    11,   169,   180],\n",
      "        [ 6285, 19999,  3966,  4948,  8179,  7652],\n",
      "        [  652,  1505,   839,   459,   196,  2447],\n",
      "        [19999, 19999,    37, 19999, 19999, 10599],\n",
      "        [  613,    16,  2251, 19999,  2251,   733],\n",
      "        [  173,  3549,   677,   212,  6362,   307],\n",
      "        [  564,   434,     0,   926,   111, 19999],\n",
      "        [19999,   855,  1720,   449, 10954,   155]]) tensor([[ 1319,  1242,  4897,  ..., 11234,   357,  3530],\n",
      "        [  243, 19999,  5131,  ...,  3717,   517,   972],\n",
      "        [  268,   183,  1548,  ..., 15248,  1801,   327],\n",
      "        ...,\n",
      "        [ 3431,  6628,    43,  ...,  6810,    37,  9255],\n",
      "        [ 9765,  1992,  6978,  ..., 13390,   629, 14177],\n",
      "        [ 2671, 12466,   303,  ...,    48,     7,  2527]])\n",
      "tensor([19999,   323,  2364,   529,   439,   844,   398, 19999,    38,   250,\n",
      "          377,    62, 19999,    38,  1973, 19999,  1006,  2335,  1548,  2573,\n",
      "         6646,   977, 19999,   350,    51, 15151,   578,  1666, 19999,    97,\n",
      "        19999, 19999,   722, 13897, 15951, 19999,  4219, 11157,    14,     6,\n",
      "        19718,     2, 19999,  1669,  2742,  5895,  5922,  2483,   270,    45,\n",
      "          308,  4658,  6517,  1393,  2742, 19341,  5224, 12836,  4048,   250,\n",
      "         8369,  1158,   221,  8888]) tensor([[ 2177,  1689,  2554,    83,    20,  1990],\n",
      "        [ 3531,  2233,    38, 10870,  1050,    38],\n",
      "        [   40, 16427,  2303,  6065,   964, 19999],\n",
      "        [  405,   558, 12632,    80,  9790,   163],\n",
      "        [  688,   105, 19999,  8567, 19999, 19999],\n",
      "        [  512,   474,   151,  1260,   474,  1552],\n",
      "        [19999, 19999, 19999, 19999,   131, 12182],\n",
      "        [19999, 19999,   414, 19999, 19999, 19999],\n",
      "        [ 1029,  4231, 10684,  9707,   985, 19801],\n",
      "        [ 5089,   409,   145,   201,    64,  1531],\n",
      "        [ 1875,   957,  5348, 14647,  3165,  4322],\n",
      "        [  135,   227, 17404,   910,  5196,  5196],\n",
      "        [19999, 19999, 17146,  2145, 19999, 19999],\n",
      "        [19999,   721,  3253,  6117,  1616,   540],\n",
      "        [ 1425,   246, 19999,   143,   783,  1063],\n",
      "        [  879, 12641,   287,  1079,     9,  3349],\n",
      "        [19999, 19999,    40,   167, 19999,  6515],\n",
      "        [ 4092,   403,   267,  4092, 19759, 19999],\n",
      "        [  267,   886,   295,  5153,  2287,    18],\n",
      "        [ 1676,  5907,  1841, 11915,    51,  1116],\n",
      "        [ 5884,  1213,  2161,   550,  1266,   260],\n",
      "        [19999,  1646,  1403,   388,   585,  1646],\n",
      "        [13189,  8729,  7143,  1760,  6595,  3616],\n",
      "        [ 7684,  7238,   181,  3019,    66,    78],\n",
      "        [ 3221, 11268,   136,  1422,  1480, 19999],\n",
      "        [10618,  1478,   536,  2085,  1387,    19],\n",
      "        [ 2712,  2911,   647,    31,  1032,    29],\n",
      "        [19999,   534,    12,  6171,  1198,   612],\n",
      "        [  774,   225, 10039,   389, 19999,   236],\n",
      "        [  437,   156, 19999,  2115,  1763, 19999],\n",
      "        [19999, 19999, 19999, 19999, 19999,  8018],\n",
      "        [15158,   479,   792,  9820,    31, 19999],\n",
      "        [  864,  2240,  6964,    22, 19999, 19999],\n",
      "        [  260,  1026,   205,  2214,  2889,   146],\n",
      "        [12887, 19999,  7479,  3405,  1369,   151],\n",
      "        [   17,   707,  3516,  1596,   817,  2263],\n",
      "        [19999, 19999,  6560, 19999, 19999, 19999],\n",
      "        [   16,  2910,   228,  4083,  8288,   425],\n",
      "        [19999,    38,  7064,   301,  3929, 19999],\n",
      "        [ 1369,   242,   442,   849,  5377, 19999],\n",
      "        [ 2050,  9469,  1113,  3290,   326,  7465],\n",
      "        [   79,  4270,  1647,   301,  6026, 19999],\n",
      "        [19999,  4591,   131,  3180, 19999,    76],\n",
      "        [  894,  1698,   429,   693, 19999,  4164],\n",
      "        [16779,   169,   847,  5642, 19999,  2180],\n",
      "        [15697, 13457,   927,  3047, 15697, 13457],\n",
      "        [ 2165,  3751, 19999, 19999, 19999, 19999],\n",
      "        [ 2483,  5723,   644,  5723,  1783,  5723],\n",
      "        [10847, 19999,   180,   938, 19999,  3529],\n",
      "        [ 9647,   688,  2963, 19999,  8428,   207],\n",
      "        [ 7924, 19314, 19999,     7,   207,    37],\n",
      "        [  119,   716,  1040,  1051,  1040,   303],\n",
      "        [ 2913, 19999, 11948,  2901,  2848,     0],\n",
      "        [19999,   247,  4390,    15,   532,  1590],\n",
      "        [ 1037,  7552, 19999,    55,    40,   168],\n",
      "        [ 1132,   386,   689,  1599, 19999,    11],\n",
      "        [ 1044,   100,   899,  1156,    22,   352],\n",
      "        [19999,  2869,  8125, 19999, 19999,  6588],\n",
      "        [  870,   482,   902,  3406, 17999,   113],\n",
      "        [  379,  2252, 12453,  8601, 19999,  5294],\n",
      "        [19999, 19999, 19999, 19999, 19999,  5746],\n",
      "        [ 1067, 11245,  1956,  1726,  5830,  6221],\n",
      "        [19999,  1521,  5570,  3016,   251,  4598],\n",
      "        [  853,  2331,  1208,  9058,  1514,   399]]) tensor([[ 4310,  1689,  4585,  ..., 15485,  7413,   770],\n",
      "        [ 2313,  1356,  2871,  ...,  8808,  1335,  5505],\n",
      "        [ 4386, 19999, 12759,  ..., 11199,  5638, 16429],\n",
      "        ...,\n",
      "        [14412,  1083,  9363,  ...,    86,   629,  1848],\n",
      "        [ 3117,  4294,   186,  ...,  5090,  2993,  1087],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        [ 3648,   509,  9311,  ...,  9360,   462,  1499]])\n",
      "tensor([19999,  4621,  1471,  2409, 12044, 13225,    36,   327, 19999,    87,\n",
      "          861,   160,   200,  4560,   703, 19174,  3586,  4599,   821,   552,\n",
      "         3119,   113, 19999,     3,   422,   197,  3878, 19999,   988, 19999,\n",
      "         1802, 19999,  6105,   224, 10410,  7800,   757,  3834,  2957,  3259,\n",
      "         3763,  3770,     0,  2139,  1117,  2136, 16374,    56, 19999,  4610,\n",
      "           16,   298, 19999, 12878,   108, 14068,   214, 19999, 19999,  3974,\n",
      "           95,  3251,  1943, 10237]) tensor([[10153,  2794,   583,    32,     4,    76],\n",
      "        [ 6228,     4,     6,  3150,   864,   321],\n",
      "        [  547, 19999,  1876,    92,  8034,   154],\n",
      "        [ 7128, 19999, 17096, 11259, 19999,  2815],\n",
      "        [ 3631, 17113,  2074,   276,  1594,  4576],\n",
      "        [   33,  1664,  6940,   114,   197, 19999],\n",
      "        [19999,  1242,   916,   916,  1703, 13327],\n",
      "        [   25,    53,  1681,  2594, 19999,   503],\n",
      "        [   29,   941, 19999, 19999, 19999, 19999],\n",
      "        [   53,     6,  2983,   700,  3555,   860],\n",
      "        [  993,   845,  2537, 19999,  7873,   385],\n",
      "        [ 8868, 19999,    67,  3565,    46,  2203],\n",
      "        [ 2248,  5990, 19999,  2366,     0,   887],\n",
      "        [  317,  3326,   246,  3007,  6549,  2685],\n",
      "        [ 3016,  2413,  1521,  1905,   525,   875],\n",
      "        [ 5364, 19999,  1991,   255, 19999, 18131],\n",
      "        [19999,  1370, 19999, 19999, 18757,  5828],\n",
      "        [    1,  1280,    57, 19999, 19999, 19999],\n",
      "        [ 1014,   584,  8137,  2090,   638,  4701],\n",
      "        [  361,  1199,  5556,  4593, 19999,   870],\n",
      "        [ 2390,   305,  4450,  5154,   558,   480],\n",
      "        [    5,   774,  1266, 16462, 12989,   599],\n",
      "        [ 5969,   648, 19999,   554,   289, 19999],\n",
      "        [   19,   320,   625,     2, 19999, 19999],\n",
      "        [  907,  4602,    74, 19999,   715,  4920],\n",
      "        [  116, 14413,   922,  9279,   809, 19999],\n",
      "        [ 1859,   928, 19999,   474,  1552,  1051],\n",
      "        [  621,  2494,  1460,  4778, 19999,   145],\n",
      "        [  988,  1049,   383,  1612,   348,  1612],\n",
      "        [    5,   248,     8, 10910,    41,  3044],\n",
      "        [ 5668,   137,  1132, 19999,    99,  1445],\n",
      "        [ 1244,  3650,   549,  2248, 14211,  2248],\n",
      "        [ 5063,   192,   600,   217,  6911,  5038],\n",
      "        [12899,   565, 10992,    92,  5075,  1005],\n",
      "        [ 2322,  2960,  5344,  1595,  4257,   660],\n",
      "        [  306,  1245, 19999,  6252,   916,   494],\n",
      "        [ 1471, 15891,   938,     9,   161,  1130],\n",
      "        [ 1820, 19999,  2601,  2087,    66,  1285],\n",
      "        [  933,  6570, 19999, 19999,   552,   789],\n",
      "        [19999,   447,  1280,  3408,  8202,  3844],\n",
      "        [ 1204,     2, 17814,  4395,    26, 19999],\n",
      "        [ 9124,   586,   118,   729, 19999,   729],\n",
      "        [ 6859,  2463,    12,     5, 19999,    10],\n",
      "        [ 2556,   757,  2524, 11899,   326,   367],\n",
      "        [   77,   658,   116, 19999,  2654,   590],\n",
      "        [19999,  8533, 19999, 11214,  4475, 19999],\n",
      "        [17991, 19999, 11276, 17594,  1069,  2621],\n",
      "        [   56,  3575,   364,   339,   403,    56],\n",
      "        [14158, 19999,   368, 13921,  9549,     8],\n",
      "        [12253, 19999,   574, 19999, 19999, 19999],\n",
      "        [  448,  2215,   521,  1049,   869,  1602],\n",
      "        [19999,    26,  5178,  1134,   377,  6529],\n",
      "        [ 8976, 19999, 14147,  8626, 19999,   799],\n",
      "        [  251,  2492,  2092, 13007,   163, 12438],\n",
      "        [19999, 19999,  2863,   183,   300,     2],\n",
      "        [19999,   633,  8301,    42,  1231,  1252],\n",
      "        [19999, 19999,  1837,  5780,   665,  8540],\n",
      "        [  470, 13412,   250,   747,  4504, 18515],\n",
      "        [11308,   608, 19999,  6818, 19999, 19999],\n",
      "        [  970,     9,  1175, 19999, 16173,   223],\n",
      "        [   91,   334, 19999,   934,   524,  1995],\n",
      "        [ 6386, 19999, 19999, 19999,  6624, 14763],\n",
      "        [ 1098,  2124,    19,  2773, 19999, 13301],\n",
      "        [  689,   557,    55, 19999,   169,   847]]) tensor([[18376,  8474, 16497,  ..., 11457, 18126,  4809],\n",
      "        [11089,   886,   190,  ..., 18356,  8187,  2904],\n",
      "        [   41,    42,     5,  ...,  7934,  2625,  7386],\n",
      "        ...,\n",
      "        [10576,  9496,  7276,  ...,  1797,   666,  1138],\n",
      "        [   57,  6616, 19999,  ...,  4227,  7980,  6594],\n",
      "        [ 2412,    86, 16172,  ..., 19999,   313, 10009]])\n"
     ]
    }
   ],
   "source": [
    "# for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "#         print(input_labels, pos_labels, neg_labels)\n",
    "#         if i>5:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss: 45.747745513916016\n",
      "epoch: 0, iter: 1000, loss: 41.274600982666016\n",
      "epoch: 0, iter: 2000, loss: 39.07835388183594\n",
      "epoch: 0, iter: 3000, loss: 32.79408645629883\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "#随机梯度下降\n",
    "\n",
    "for e in range(NUM_EPOCHS): #开始迭代\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        #print(input_labels, pos_labels, neg_labels)\n",
    "        \n",
    "        # TODO\n",
    "        input_labels = input_labels.long() #longtensor\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "        if USE_CUDA:\n",
    "            input_labels = input_labels.cuda()\n",
    "            pos_labels = pos_labels.cuda()\n",
    "            neg_labels = neg_labels.cuda()\n",
    "       \n",
    "        #下面第一节课都讲过的   \n",
    "        optimizer.zero_grad() #梯度归零\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        #打印结果。\n",
    "        if i % 1000 == 0:\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
    "                print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
    "            \n",
    "        \n",
    "#         if i % 2000 == 0:\n",
    "#             embedding_weights = model.input_embeddings()\n",
    "#             sim_simlex = evaluate(\"simlex-999.txt\", embedding_weights)\n",
    "#             sim_men = evaluate(\"men.txt\", embedding_weights)\n",
    "#             sim_353 = evaluate(\"wordsim353.csv\", embedding_weights)\n",
    "#             with open(LOG_FILE, \"a\") as fout:\n",
    "#                 print(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "#                     e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "#                 fout.write(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "#                     e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "        if i > 3e3:\n",
    "            break\n",
    "    embedding_weights = model.input_embeddings()\n",
    "    np.save(\"embedding-{}\".format(EMBEDDING_SIZE), embedding_weights)\n",
    "    torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"embedding-{}.th\".format(EMBEDDING_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 MEN 和 Simplex-999 数据集上做评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simlex-999 SpearmanrResult(correlation=nan, pvalue=nan)\n",
      "men SpearmanrResult(correlation=nan, pvalue=nan)\n",
      "wordsim353 SpearmanrResult(correlation=nan, pvalue=nan)\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = model.input_embeddings()\n",
    "print(\"simlex-999\", evaluate(\"simlex-999.txt\", embedding_weights))\n",
    "print(\"men\", evaluate(\"men.txt\", embedding_weights))\n",
    "print(\"wordsim353\", evaluate(\"wordsim353.csv\", embedding_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 寻找nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'这'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-1790a3bca792>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#     print(word, find_nearest(word))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'这'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'电脑'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'美国'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfind_nearest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-71c23c5e00b0>\u001b[0m in \u001b[0;36mfind_nearest\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_nearest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mcos_dis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0membedding_weights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '这'"
     ]
    }
   ],
   "source": [
    "# for word in [\"good\", \"fresh\", \"monster\", \"green\", \"like\", \"america\", \"chicago\", \"work\", \"computer\", \"language\"]:\n",
    "#     print(word, find_nearest(word))\n",
    "for word in ['这', '电脑','美国',]:\n",
    "    print(word, find_nearest(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 100) (30, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD2CAYAAAA6eVf+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeVyU1f7A8c+ZgZlhXwQEdxNzKVsQ\nSkvLzKWrpteU3CvNsiyz7Fpp3Z+WppV5y3ZcKs0iNa1cS83d1EQzRCuX1NxIENlkGYY5vz+AkU1F\nZJXv+/V67mvmmfM8c568fDmc7au01gghhKgZDJVdASGEEBVHgr4QQtQgEvSFEKIGkaAvhBA1iAR9\nIYSoQZwquwKX4ufnpxs1alTZ1RBCiGpl165d8Vpr/+I+q9JBv1GjRkRFRVV2NYQQolpRSh272GfS\nvSOEEDWIBH0hhKhBJOgLIUQNIkFfCCFqEAn6QghRg0jQF0KIGkSCvhBC1CAS9IUQogaRoC+EEFVM\nZmYmhw8fLpd7S9AXQogqZu7cuQwbNqxc7l2lt2EQQoiaJi4ujhdffBGlFD169OD06dMEBQVht9tJ\nT09n/fr1V3V/aekLIUQVkZWVRXh4OHa7nRtvvBGAP/74o0y/Q4K+EEJUEUePHuX48ePceOONeHp6\nAjn9+3a7HZvNRp8+fa76OyToCyFEFeHs7MygQYP4+++/0VqXy3eo8rpxWQgNDdWytbIQ4lq3c2kE\n9XdPo1b2Gc4of1r8L5Yz8WexWCy4u7uTmpoKgN1uR2uN0Wi85P2UUru01qHFfSYDuUIIUYl2Lo3g\nxl2v4KKsfLjLytI/j+Js19zbLgyfwIZkZGTQo0cPAGw2G//+97954oknSv190r0jhBCVqP7uabgo\nKwAGpVBK0baeEdfkv4qUVUphMFxd2JbuHSGEqET2CV4YVDHntcLwaiIuLi6kp6df0T0v1b1T6l8Z\nSqk5SqltSqlXLlGmtlJqc773dZVSJ5RSG3KPYnM4CiFETXHmImHwjPID4LrrrivT7ytV0FdKPQAY\ntdZtgeuUUk2LKeMDzAXc8p2+HXhda90h94grzfcLIcS14njIWNK1qcC5dG3ieMhYAPbt21em31fa\nln4HYGHu69VAu2LKZAP9gOR859oAw5VSu5VSU4q7sVLqcaVUlFIqKi5OficIIa5tYT1HENN6MrH4\nY9eKWPyJaT2ZsJ4jyuX7Sjt7xw04mfs6AQgpXEBrnQw5Aw/5rAImAWnAWqXUTVrr6ELXzQRmQk6f\nfinrJ4QQ1UZYzxGQG+QDc4/yUtqWfirgkvva/Qru87PWOkVrnQ38ChTpFhJCCFF+Shv0d3GhS+dm\n4GgJr/tRKRWklHIFugAxpfx+IYQQpVDa7p3vgM1KqTrAv4D+SqnJWuuLzuTJ9SqwHrACn2it/yzl\n9wshhCiFUgV9rXWyUqoD0Bl4S2sdC/x2kbId8r1eDzQvzXcKIYS4eqXehkFrfY4LM3iEEEJUA7IN\ngxBC1CAS9IUQogaRoC+EEDWIBH0hhKhBJOgLIUotL7kHwJEjRyqxJqKkJOgLIa5IkyZNiIiIAOD6\n66/n2LFjALRq1Yp169ZVZtVECUjQF0JckeXLl/Pmm29it9t57rnnmD59OitXrqR27dp07Nixsqsn\nLkOCvii1zz77jJiYnJ00kpOTsdvtlyxft25dfvzxRyAn1+dPP/1U7nUUZWvfvn388ccfrFixgv37\n9zNu3Djmz5/PoEGDiI+PJzg4uLKrKC5DcuSKUjOZTLRt25YVK1YwaNCgAtl9srKySEpKIjU1lTp1\n6uDk5ERSUhLh4eG4u7vTqlUrzp07xy+//FKJTyCuVExMDNOmTcPX15d33nmHOnXq8PfffwPw559/\n0r1790quobgcCfqi1AYNGoTFYsHb25vjx48X+Mzb2xsAd3d3kpNzUio0aNCAr776imnTpnHy5Ekk\nFWb1sXNpBPV3TyNcx/GnnzNrzmaSmprKyZMn8fX1BXL+evPx8ankmorLkaAvSuWFF15gyZIlWCwW\nRxdPYZGRkYwcORKTKScrUFpaGnfddReurq64ublRq1YtHnjgAWbNmlWRVRdXaOfSCG7c9UpO8m4F\nnqTikfQn1mO/cO7cOYKDgxkyZAhJSUnMnj27sqsrLkP69MUV2bk0gtiJwbzhEsGWwXDsyF8AdOrU\nCRcXF7y9vfH29kZrzYABAzh37hy///47L7zwAtnZ2XTu3Jk5c+bw3nvvceTIEQn41UD93dNyAn4+\nhxNs9B76LJGRkTRo0KCSaiZKQ4K+KLG8Fl8gcRgUBBKHMTudnUsjsFgsTJ06lcTERBITE3nmmWew\n2+188MEHNG/enMmTJxMeHk5gYCArV67ksccek+6daiKgUCrrv87ZOZSgiXzAwqRJk3jssceAnO6d\nmTNn0qdPn8qopighCfqixIpr8QEsfnc8WVlZBc5NmjSJuXPnMmPGDIYOHUpKSgrLli1zHOfPn3d0\n+4iq7YzyL/D+8RBnPviXmUeXW6lfvz4jRozgxhtv5IsvvuDpp5+meXPZPb0qk6AvSqxwi2/nSRvJ\nmRC5+xx2u52kpCTHZ6NHj2bbtm0cPHiQG264gZYtW5KQkOA4GjduXNHVF6V0PGQs6frCL+gbazvx\nSKgHjz4ymG3btgEwdOhQsrKysFqtvP7665VVVVECEvQrwJ9/ln2CsPzL3/OUZK781Sjc4vsz3k5I\nkIFtzzSiQ4cOTJo0ydGn/+GHH9K2bVsAzp8/z/79+/H19XUcR44cKTDFU1RdYT1HENN6MrH4Y9eK\nWPyJaT2ZCe98VtlVE6WgtNaVXYeLCg0N1ddCv6+7uztr1qxxBME8BoMBNzc33NzcuOmmm9iyZQsW\niwW73Y63tzdHjx4tUL5Dhw5EREQQGRnJd999x549ewp83qRJkwKt7fzOnTtHdnb2VT1HgVkcudK1\niZjWkwnrOeKq7i2EKDtKqV1a69BiP5OgXz48PT0xmUxkZ2eTmJhIrVq1HJ9lZ2czePBgFi5cyKBB\ngzh9+jTJycmkpaUREhLCuXPn2LhxI4cPH3Zcs2XLFnr27ElCQgJWqxVvb2/27t1LkyZNSlQfZ2fn\nIv3upZE3XztAx3NG+XE8ZKwEfCGqmEsFfZmnX07yFiS1aNGCgQMH8uGHHxYps3DhQpYsWcL8+fN5\n44038Pb2pn79+pjNZpRSBcr27duXd955B8hZCTtq1Ci6dOni+MUwceJE3n77bZycCv6T2mw2Nm3a\nVGbPFdZzBOQG+cDcQwhRjWitq+zRunVrXZ298MILWimlfXx8HIerq6u++eabtdZaBwQE6CZNmuiY\nmBjdvXt37eLion18fLSXl5du2LCh4z7du3fXISEhWmutR44cqf/55x+ttdZNmzbV9957b4nq4uTk\nVLYPJ4SosoAofZG4KgO5ZShv4ZJ9ghfPtPfhf9On4+rqytNPP+047r777gKt8dmzZzN48GAApkyZ\ngp+fH4mJiQwdOhSA6dOns2XLFjZu3MiOHTv4/PPPHVsc7N69m/379/Pxxx8DEB4ejtlsdgyWms1m\nevXqVcH/FYQQVVmpg75Sao5SaptS6pVLlKmtlNqc772zUmqZUmqrUmpYab+7Ksq/cGl+tJWvdyfx\neW9X7Nk23N3dHYfZbC5w3S233FJgwNbFxYUWLVqwZs0aMjIyeO655zhw4ADu7u4MGzaMSZMmYTKZ\nePHFFzGZTJw6dYonn3wSAC8vL/r06eOYFtm/f3+ZCy+EKKBUQV8p9QBg1Fq3Ba5TSjUtpowPMBdw\ny3d6FLBLa30n0Fcp5VGa76+K8i9ceuhmE2fGetDQI5uMjEzeeOMNx7F69eoC111//fWOlj7Ayy+/\nzPnz5x0zeQwGAwEBAXzzzTckJSUxZswYAKKioggPDy9Sj8WLFzta+l9//fVVz9gRQlxbStvS7wAs\nzH29GmhXTJlsoB+QfJHrNgFFRpeVUo8rpaKUUlFxcXGFP66yCi9cyuPnqgosSpoyZYpjkNZutxMd\nHc37779PRkYGn376KUuXLsXHx4ewsDASEhIA2LBhAwMGDMBisdCiRQuCgoLYt28fK1asYMeOHY7v\nSk1NJTw83PFdCxcu5IEHHnDcRwghShv03YCTua8TgNqFC2itk7XWhSeNl+S6mVrrUK11qL+/f+GP\nq6zCC5cAMm2guTAL57PPPuM///kPnTp1AnKCfmBgzvyXv//+G5PJxP/+9z927tyJzWbj/fffB3KS\nj7Rv356BAwfy7rvvcvjwYWJjYxkzZgxz58513P/rr79m/vz5jve9evXCYrHg7+9PWFhYuTy3EKJ6\nKdU8faXUDCBSa709t6unudZ6ykXKbtBad8h9/T0wQmsdq5QaA8Rqrb+62PdUp3n6V7twKSMjA4vF\nUp5VFELUEJeap1/alv4uLnTp3AwcLefrqryLLVUv6cIlCfhCiIpQ2pa+J7AZ+An4F9AfCNdaF5nJ\nU6il3xBYCawF7gDaaK0vOtJYnVr6QghRVZR5S19rnUzOoOx24B6t9W/FBfzcsh3yvT4GdAa2Ap0u\nFfCFEEKUvVJvw6C1PseFmThXct2p0lwnhBDi6smKXCGEqEEk6AshRA0iQV8IIWoQCfpCCFGDSNAX\nQogaRIJ+FWOz2cjIyHC8t9vtZGRkkJCQUK75b4UQNYME/QrWqFEjPDw8CiQJ9/X1xWg0OjJp1atX\nD6PRiK+vLxaLhbvvvptGjRpx3XXXERQUxGOPPVbZjyGEqKYk6Fcwo9HIggULCuy8mZCQQOPGjXF1\ndeXBBx+kZ8+eaK3Jzs7GyckJJycnQkJCePbZZwkJCWHWrFlX9J3JycmsW7eunJ5ICFGdSI7cCqaU\nol+/fjg7Oxc4n5SUREZGBgMHDiQyMpLbbrsNb29v7r33Xs6dO8e2bduYOHEiS5YsueLvfPbZZ1m+\nfDlnzpy5aBmz2YyLi0uBcykpKSxYsIC+ffsWqL+HR8E0CGlpaYSHhxMZGXnFdRNCVCwJ+pVgwYIF\ndOvWjXHjxnHgwAEWL17s+Kxu3bqsXLmS3377Da01p06donnz5hw4cIBbbrmFZ599lieeeIKRI0eW\n6Lt+//135s6di7u7O7Vr1yYlJQUPDw+01mRlZXHu3DkgJ2NXYmJigWuDg4Px9fUtcM5kMjmSvufp\n1KlTkV9iQoiqSYJ+Odu5NIL6u6cRoOM4o/zJPJ/EgAEDcHZ2JjMzk4yMDPz8/NBa4+fnR9euXbnl\nllsceXQPHjzItm3baNy4MU5OTnh7e3PixIkSfXdaWhrt2rWjc+fO/PDDDwC4u7vzzz//FClrtVod\nuXfzpKSkFEm3mJcApjCj0ViiOgkhKpcE/XJUYI99BYHEYcpM5dXRT/Lsax9gs9nw8/MjPj6+yLW1\naxfML3Po0CG01pfsoils9+7d3HDDDfzzzz+O+6WlpREQEEB2djYdO3Zk0aJFQM7WzoUzbAUHBxe5\nZ3G/HM6fP8/DDz9c4noJISqPBP1ylD9vbh6Fxu/wYr75pgPdunWjWbNmPP/880yfPr1AIpX27dsX\naWVnZmZe0fe3a9eOTZs24ePjw+nTp7FYLLi7uxf7iyMrK6tE9zSbzUW6gfIygQkhqj4J+uUoQMdB\nod6Qcxma0d/+g8vGZ2nTpg3ffvstLVq0wGq1sm7dOh588EGGDh3Kt99+W2TANCUlhSNHjtC4ceNL\nfm9el5J31j+cUf5k26zFlrPZbNjtdkwmE2lpaUX675OSCme7FEJUdxL0y9EZ5U8gBROm1/c08EQ7\nP5745kK//O7duwkPDycjI4PRo0dfMtheboFW/i6lAd9nsOavI1iUok5gAM5mF9LS0hxdPXa7na5d\nu/L444/j5+dXpK+/uO6di8nOltQIQlQHEvTL0fGQsXgVypu7bYQvMa1fK1CuSZMm7N692/He29u7\n1EE0f5dSZF9Xx/lY/AmceKjYgdy2bdty//33F7lXdnZ2kYHbjIyMYvv0BwwYUKr6CiEqlgT9chTW\ncwQ7IXf2TjxnlB/HW48tcd7c0iiuSynnfM5gsc1mK3B+06ZN/Pbbb/z000+Oc8nJyQQFBQHQqlWr\nAuWdnZ2L7dMv6ZiAEKJylSpHbkWRHLlXLnZicJEuJbjQ0i9OWloarq6uBc7ZbDbHtFEhRPVS5jly\nRdV1PGQs6brgrJ90beJ4yNiLXlM44AMS8IW4RknQv4Tk5ORqt7NlWM8RxLSeTCz+2LUiFn9iWk8u\n1y4lIUT1UWO6d1JTU3FycsJisWC1WunRowdbtmxxzItPTEzEbrezZcsWhg8fzh9//EGjRo1ISUlB\nKcXZs2e57rrrOHz4cJnURwghyku5dO8opeYopbYppV4paRmllJNS6m+l1Ibco9XFri1rc+bMISAg\nAKPRSJ06dTCZTEyZMsWxy6W/vz+Qs/gob7Dz6NGjnD17liZNmnDvvfdKwBdCVHulCvpKqQcAo9a6\nLXCdUqppCcvcBERqrTvkHnuvpvJXYvTo0ezcuZPGjRsX2PagQYMGbNmyBYCPPvqI0aNHc/r0aQYO\nHEhGRgY333wzgYGBrF27tqKqWiHytlwoPJtHCHFtK21LvwOwMPf1aqBdCcu0AXoopX7J/SugyGih\nUupxpVSUUioqLq7oLJTSmj17NqGhoRw5coTrr7/ecT4zM5OQkBAAwsLC6NKlC+7u7vTq1Yt69eoR\nHR3NunXrMJlMuLi48MYbb5RZnSqa3W6ndevWANx555289dZb9OvXj0GDBgE5XWB5WbusVitdunTB\n1dXVkejFYJAhICGqu9L+FLsBJ3NfJwC1S1hmJ9BJa30b4Ax0K3yR1nqm1jpUax2a1+VyNXYujSB2\nYjDDjj/Pqod9qBPgy4EDBxwtXJPJhKurK9nZ2YSFhdG9e3c8PDzo168fJ06cwNfXl48//pg6deoQ\nGhpaZD+c6mTBggWkpKSQmprKqVOnGDlyJLNmzWLz5s3Ex8eXuAtMCFF9lTbopwJ5GTfcL3Kf4spE\na61P556LAop0C5WlvC0JAonDoMCfBEyZCexcGsFzzz1H27ZtOXv2LOPHj2fGjBkFrp03bx4RERHU\nqlWLJUuWcM8993D+/Hn8/PzKs8qldvjwYd5///0C5zIyMgp034wbN46DBw8SFBREcnIyDRo0oG7d\nupw8eZImTZrQqVOny3aBCSGqt9IG/V1c6NK5GThawjJfKKVuVkoZgX8Dv5Xy+0sk/5YEyRl2lh+w\ncSLZTpd+I+natSvvv/8+wcHBrFu3jkGDBrFp0yaef/55jh07xksvvcR///tfPv30U1avXs2bb75J\nenp6lWjtLl68mG7dutGjRw+GDx8O5GzFPH78eI4cOeIoN2rUKGrXro2fnx9+fn6cPXsWyOnGMRgM\nJCQkcNtttzFmzBj27dvH+vXrL9sFJoSo3kob9L8Dhiil/gc8COxTSk2+TJkVwGvAF8AeYJvWulxH\nRwP0hTGBh79L5+1tVp693cSfT7sxd+5c1q1bx+OPP47dbmfatGlMnTqVW2+9lZSUFLZu3cry5cvp\n06cP/fv3x9PTk4SEhCvahKy8NGvWjF69epGcnMz27dvx9fV17LzZpk0b/Pz8eO2113j8/lD2jfLh\nzFNZxDztjUFpR7+83W7HYDCwefNmpk+fTpMmTQgKCiIqKorGjRtftAtMCFHNaa1LdQA+5ATzwKsp\nc6mjdevW+mqcntBE6wmeRY4eLd10/fr19cmTJ/WWLVu00WjURqNRv/322wWub9WqlR46dKjWWuug\noCDdrFmzq6pPWTpw4ID29fXVmZmZWmut27Ztq5977jm9a9cunZmZqX/5/hOd9n9+jme+o55BA9rF\nYtZBQUHaaDTqWbNmaWdnZx0cHKxvvfVWrbXWf/zxh27SpInWWusffvhBb9++Xbu4uOhx48bp+fPn\nV9rzClGeXn31Vd2zZ089a9asyq5KmQCi9EXiaqnX2mutz3Fhdk6py5Sn4na5TNcmxvx3PPf0fw6A\nOnXqXHTaYnR0tOP1qVOnyreyV8hkMuHm5oaTkxPffvstJ06c4I033iAgIIDPPvuMW3e9ibMhA1RO\nyz7dpvE0Q7o1kzp16nD69GlGjhyJ3W6nb9++7Ny5k+TkZJYvX87x48fx9fUlISGBwYMHO7rApkyZ\nUslPLUT5MJvNmM3maj1Ro6Su+RW5F3LU5u5yGVK+u1yWp/z5dn8+60WHT05y2223c/LkSc6cOUNm\nZibDhg3jkUceYdbT97DiwIVfZokZkK3BSYFN5+yWmZWV5fjFMX78eLZt28b27dt56KGHGDt2LCtW\nrGDcuHGMHz+eefPmER4ezvr161m5cmUl/lcQomy0aNGCv//+G7PZTEZGBtnZ2Tg7O+Pk5IRSittv\nv92RW7q6udSK3FJ371TEcbXdO9eS/N01E+82aRcntK+L0psWvae11rpv37767rvv1r/88ou2WCx6\n2bA6Bbq0ujc16s6NDdrZgFagTSaTNplM+qmnntKANpvNev/+/Y7v69u3b4m6wISorlq1aqVbtWql\ntdZ62rRpun///vqLL77QR44c0R07dtTdu3evlHqZTCbt5eVV4DAYDHrRokUlvgfl0b0jKlb+mUhP\n3Wbi0RATd8w5j33dW9QaMZGOHTsyceJE7rrrLj777DNquyahd71A/hwoBqUIq2PgaJLmZHImzz33\nHMuWLcPf35/AwECaNWvmKDtnzhw8PT2BS3eBCVHd5P3FbDhzBBtO7FwaUdlVKsDFxaVIzorg4OAi\n6UxLS4J+FTZw4ED69etHr169CNBxxJyxsfnvbNYdzSY7W6MUtPdP4ccf1zJnzhz+/e9/c8sttzB0\n6FDeffddgpUHPqQAkJU78UYDu8ZcB8C6deto2rQpK1as4Prrr2fNmjV07doVwBHwhbiW5E8narPD\n0UQrXfs9SZrdCY1i2bJlODk5YTAYaNOmTaXU0Wq1FslOl5KSUmbjDRL0q7CHH36YPn368OijjxL5\n6Xk8nO2cTdO83N7EhA1WOl9nxH1KMumvhWE0Gmnfvj3r16/Hzc2NdevWETLov9y46xXumJnA73F2\nVgx05Y7GrsSEjCUQ2Lp1K+7u7gAcOlR8ghUhriX5/2L+tFfO2tHb6hqZ8LMzB2p1pnv37vTt25e9\ne/diNpsrpY4Wi8WxN1aespwqLkG/irLZbAQFBXHmzBkMBgMD72lO6vcvMeL7FMbeaWH07U5kKgt/\nhE2hRcdBuLu7s2HDBho2bIjNZmPz5s2sX7+e+9rdyqoRfzkGsmPyDWTnBXwhaor86URvq2t0nHe1\nn3f0eVssFsLCwiqkPvknZ5xR/hwPGVvuqUcl6FdRGzdupEePHvTv359FixZhMpnISM8gI0Pj+2Yy\ndgy0aNGIAx+Nx25/CYPBgMVi4eTJkzRr1ozPP/+c9957j6AGDQicuBqAwNxDiJrqjPIvNp3oP1ku\nnD9/HqvVWsxV5SN/VxMKAonDa9crpKWdL9J/n5SUVGbfK9smViF5m8P1buHEB8/2YtGHE7BYLGRk\nZDBz5kw8vXxQBgPa4o3BxYsde35nzpw5WCwWDh8+zLFjx4CcjF/NmjUjKSkJHx+fSn4qIaqOi6UT\nHfD8VJYtW8ajjz5aYXXJ39WUZ+exNHxdlGOTw7wjb8V9WZCgX0lmzJiB0WjE29sbo9FYYHM4Z4PC\ny5hJ+8PTmDf3c5o3b054eDjx8fEYDAaSkpJITExEa014eDgpKSl06NCBVq1aYTKZ+OeffwgODmb1\n6tVMmjQJZ2dnRo0aVdmPLESlq0rpRPNvE5PnxbUZ3N/UWOR8dnY2Kv9UvKsg3TuVxGQyccMNNxAd\nHY2bmxvTJ/yHg7FpGA3w1zk7SsHBeYmkp9uZNGkSDzzwAEopXF1dMRqNZGdnk5ycjFKKvn37Eh0d\nzYEDB7jllltQShEVFcWtt97Kyy+/zAcffCCJzoXIFdZzBOQG+crs8izc1bTpqI3f/rHz5UMNHeeS\nk5MJCgoCoFWrskk0KJGgkjg7O7Nv3z68vb3JzMzkq55u7Dxp4fZ6TgxcnIazARbus+HiBC1btgTA\nYDBgNBo5d+6c4z7Z2dn8+OOPREZGMnbsWKKionB3d8fX15fExESGDRtGenq6I3mKEKJqKLxNzF2N\nnDj+H1/+avMi1+WW8fT0JCkpqUwbbdK9U4Hy+uztE7w4v3w8jesHkpiYiMVi4c90HzrMTePvJDsA\ncWmaRt7gZFRkZmYWuZeHhwfu7u6cPXsWu91Ohw4diIuLo379+vzzzz9orQkODiY5OZk777yzoh9V\nCHEZxXU1/dV2SpGuprL+K11a+hWk8Ei9uz0ZS7qN0JuaY7fbSW03jt4/jOb+r85T291AAy/F5w94\n03RGEi0X3YmzAe67K5Tv12zFYDDw999/Fxmk7d27NwcPHsTV1bXAXwNCiKqpMrqaJOhXkMIj9dZs\n2HcmG878CUCnIS/w+vMjiJzwHs7x2SgFp1JSSc/SPPRdBll2WLr2Z8dgTqNGjQBIS0tz3NNgMLB6\n9eoi071SUlKke0cIAUj3ToUpPFK/42TOvgibH3HDYDAwffp0Zn6znpEjR+Ll60eLABN3NzKQrWHV\nQRvuJjAbIerZBiil+L//+z8ANm3ahIuLi+O+Xbp0KTLdS7p3hBB5JOhXkDOqYJrF4bc6M/YOZ4Ib\nBjJv3jxmzpzJrbfeirOzM4mJifRppunR1AmDgj4tnUgZl7MXzr0f/43Wmueffx6ARx99lI4dOwI5\nO6bmtfTzH1u3bs1LaiOEqOGke6eCFB6pn7U7i8gYGxHRpzE658yhP3ToEFpr3N3deXVTIuuP2mni\no1h/JJv5v1lxcQaTkxFltbNlyxZ69OjB/v37CQ0NZefOnWRlZdG9e3e+/fbbAt/doUOHcl/aLYSo\nHiToV5CwniPYCQTvfg1vncoN/kaeudPCytO1aHbjrQXKJicnU8/HmVYpW1j/VwZpWXBjgIHzVsg2\nmoF0evToQWJiIu7u7pw4cSLnOy6yX8iGDRvK9+GEENWGBP0KZtFWlIIX2uXs4PfB5GMcP3UGo/OF\npeFubm78dPIkO5dGcPiNF8lMTyMwKIit3+Zslubt7V1kFz4hhCgJCfoVqLi9NgwK2tVXWFrcU+D8\nsWPHCOs5gq83HuC3bdt46bxd4DcAACAASURBVOj1LHt/PFqPIzk5GV9fX7Kzs1mxYgXt2rWryMcQ\nQlRjEvQrUP5tXfMYFPRvbkf17u049+KLL3Ls2DEiIiKYNWsWwcHB9OrVi88//xxAWvpVwIwZM9i6\ndSsdO3bkiSeeqOzqCFFipQ76Sqk5QEtghdZ6cknLlOS6a1Vx27pm2eGZHzJRG55xnEtJScFms3Hf\nfffx9NNPU6dOHR577DHHDoB5LX2ATp06sXDhwop7iBrG39+f5ORktNb4+voSGxsL5KySdHZ2xtnZ\nuZJrKMSVKVXQV0o9ABi11m2VUp8qpZpqrQ9ergzQ6nLXXcsKz+ABSHzZr0S7/H344YdERERgMFyY\nZWu32yV3bTm76667CAoKYseOHcTHxzvWQZw9e5aUlBRiYmIwm80MHjy4sqsqRImUtqXfAchrXq4G\n2gGFg3dxZW693HVKqceBxwEaNGhQyupVTXkzeHIy5eRksjreemyJtnUtLj+mwWAos7yZonibNm1y\n7GZau3Zt5syZA8CKFSvYtm0b99xzjwR8Ua2UNui7ASdzXycAISUsc9nrtNYzgZkAoaGh19yKoqqy\nrau4uPwp7NoGZGO5rS1/n83AbrfTvXt3tNYMGzassqspRKmUNuinAnlr/90pfmVvcWVKcp0Qlabw\nxnhmnYlv3A72Jwbi5hPAqFGj+P333x1ZyoSobkob9HeR0zWzHbgZ+LOEZU6U4DpRAna7vUD/vigb\nhafVbjiWTXKmDTiKf2AWjz32GACffPIJ27Ztq6RaClF6pY0a3wFDlFL/Ax4E9imlCs/EKVxmxUXO\n1Xh5G6ZNnToVJycnvL29HWkUV65cCcCRI0cICgpyDNzWrl2bTZs2lfg77HZ72Vf8GlR4Y7z2DYw8\nFuLMuHZmR8CHnH2OZD8jUR2VKuhrrZPJGajdDtyjtf5Na/3KZcokFXeu9FWv/uLj44mJiUEpxb59\n+0hPT6ddu3YkJiaSmJhIcHAwFosFq9VKw4YNadasGUuWLGHlypU4OzvTp08fPDw8cHJyYt26dRgM\nBry9vXF3dy8yCB4YGMjixYsr6Umrj8Ib403vYua/d5l5okM9JkyYAMCiRYt4/fXX2bhxI02bNq2M\nagpRaqXuH9Ban9NaL9Rax15JmZJcV1NMnz6d2267jfT0dMLCwopslAY5s3YefvhhfH192bNnD0OG\nDKF3796kpaVx9uxZgoODadmyJSaTCX9/fxITE/nyyy/x9PR03GPQoEHExcUxbNgwvL29cXFxwWKx\nEB8fX5GPWy0cDxlLur4wI6qxjxFPNwvHQ8Y6zvXo0YOYmBgOHDgg21aLakc6hSvR1KlTmTdvHgCp\nqak8+eSTbN261bEl8qFDhwCIjIx0tP6HDBnCkCFDSExMxN/fn1OnTvH777877rlq1Sr69evH/fff\nD8DQoUPZuHEjo0aNonbt2ixevBgPDw9WrFiBn59fuT5f3qrh6rSWoLgUdoXXUbi4uODt7Y3ZbK7E\nmgpRSnl9k1XxaN26tb7W/PL9J/r0hCY6+/889ekJTfSNzRprQAcEBOinn35a33333Y6y119/vd68\nebOuVauWDgwM1IGBgdrFxUWbzWZdq1YtrZTSnp6eun79+nrz5s3aYrHo7t27627duumePXvq7Oxs\n/ddff+mUlBQdERGhfXx8tFJKDxgwQB86dKhcni87O1uHhIRorbVu3ry5fvPNN/UDDzygBw4c6CgT\nExOjX3jhBa211t26ddMPPfRQudRFiJoKiNIXiavS0q9AedMBA4nDoOBcXCwnjx3FxWLm3Xff5Y8/\n/mDLli2Ogdy8ln58fDynT5/m9OnTmJwMGOxWdgzMpJarYu0Xb+Ht7Q2AUoq1a9fi5eXFTz/9xA8/\n/EBoaCg33ngjy5Yt44YbbmDmzJkEBQXRu3dvgoKCePnll8v0GRcsWOB4ferUKUaOHElkZCTbt293\ndCd98sknjkFok8kkWxkIUZEu9tugKhzXWkv/9IQmWk/wdBzvdjXrWfebtauz0lpr/eGHHxbb0tc6\npwV9c4tgfVtdJz29i1m7OKFNRrSXGW1QSm/evFkHBAToBg0a6PXr1+vrrrtOa6311KlTtaenp/by\n8tIGg0G7urpqLy8v7eXlpSMiIsr8GRs2bKjNZrP29vbWSint4+PjODw9PXVMTIwOCAjQLi4u2svL\nSzs5OWmTyaS9vLy0h4eHHjBgQJnXSYiaBmnpVw2FpwOObmNmeIgZRc7Uv+zs7CItfavVyjPPPIOv\nry/+tlPsGO7KmLZm3uxkwpoNSkEdzwv/jAMHDqRbt24MGTIEgGeeeYbTp0+zY8cOTCYTSUlJJCYm\n0rBhQ7Kzs8v8GX/++WeSk5MZPHgwvXv3LpCrNykpiV9//RW73U5aWhqJiYn06NHDMUaRnJzMV199\nVeZ1EkJcIEG/AhWeDpjHZs/Zb7m4KZtpaWmkpqaybt06fhxwYS2dXSta1ILHQ0y4Gu04OzujteaG\nG24gIyODgwdztjRydXVl+fLlhISEMGrUKJycnEhNTeX06dO0atXqqp9p59IIYicGY5/gRezEYE5G\nLQNg9uzZ/Pjjj45BaQ8PDw4ePMjZs2d56623sFqtRe6VkZFx1fURQlya0lV4gUloaKiOioqq7GqU\nmQJL/HOlaxM/1hvNvx97hbS0NNLS0i46qyZ2YrBja+aRK9IJq2Nk7xk78/dm83d8KoGBgbi6urJx\n40aeeuopOnfuzNtvv42rqyuvvvoqDz30ENu2baNTp04EBwfz22+/lcvz3LnYh9iEVAICAtizZw9L\nly7lv//9r+P7rFYrtWrVwmg0cv78eQwGAy4uLjg5Ock0UiHKgFJql9Y6tNjPJOhXrAubeeXushlS\ncJfNbdu2cfvttxe7xcLFguyltma2Wq3lthNn/l9CefosOM/WE3Aqycpdd92Fj48Pv/76K9u3b6de\nvXpF7tG7d29q1arF7Nmzy6WOomTc3NzIyMjAzc3Ncc5qteLq6srrr7/Ok08+WYm1E1fqUkFfMmdV\nkGnTpvHdd9+xdetWag0dT053ejzwIkbjeM6ePQtA165dmTFjBvHx8axbt47//Oc/PPLIIxw/frxU\nWzOX59bLhTOBzdmdyaFzmr+ecSM+Ph4/Pz+WLVtG3bp1OX78OPXq1XPkALhYvTIyMjCZTLKvUAVr\n2LAhAPv373ecmzx5MlFRUTg5SZi4lsi/ZgXIyMjAYrHg7OyM1WrFaDQWWL5/5MgRAHbs2AHkLKh6\n9913MRqNmM1mlLoQWavS1syFM4E9GmJm0E3ONHg3DTW7FQ888ADp6em88cYb9O7dm6SkJD755BNG\njx5dJKgvWbIEyBnM/v777+nQoUNFPkqNZzQaiz2vlLroZ6J6ku6dCvDcc88xa9YsrFYr7dq1Y+/e\nvcycOdPx+YgRIzhz5gy33nor2dnZtGzZku+//x6bzYbZbCYtLQ0PDw/Wrl1LWFhYJT5JQRfrbtp6\n/Ut0GvR8kfI2m01ajVVUrVq1SE5OLtC9k5mZiZubG2+//TaPPPJI5VVOXDHp3qlk77zzDk2bNmXh\nwoWsXr2aBx98sMCiqHbt2jF//nz27t1Ly5YtmTdvHh999BFr167lpZdeYuDAgRw9erTKdXlcrLup\n00W6myTgVy35k8UEOWdQq34gB/467vh88uTJ7N69uxJrKMqD/BSWk/w/UGeUP2u312Hr1pxB2r/+\n+guTyURWVhbOzs5s2rQJJycnnnzySTZu3HjR1IhVUVXqbhIlVzhZjBEblrTT7FwaUaL0naL6qpqR\npJorvN3Ck18fZeuWLbRs2pBdu3YxePBg9u3bB8A///zDiRMnWLhwoaPrpl69erz00kusXr2aLl26\ncPLkSdzc3Bg9enRlPpa4hhROFqN1zv/U3z2tQDmtteRiuMZI0C8HhX+gvu3vxoS7Tbil5aQH/uKL\nL/D19SU9PZ2AgABCQwt2vZ04cYLXX3+djz76iLS0NO677z6OHz/OjBkzKvQ5xLWr8OrwY0l29sfZ\naf7GYceK8Ndff53NmzeXy8ptUXkk6JeDwj9QeZx1Ft9//z1JSUn07dsXi8XCmTNniImJKVBu3bp1\nvPzyyzRv3hzImdFyyy23lHu9Rc1ReHV4bTcD1/kY+OOlJo4V4S+//DJ33HFHtdoaW1yeBP1yUPgH\nKvofG8//mImbq5nBgwczd+5cEhISSE1NxdXVFYvFwiOPPEJqaipZWVn06dOHDz74gHbt2gHwww8/\nYDabWbduXWU8jrgGFU4Wc2CUO7897VsgWcwrr7zC0qVLZWHWNUambJaD4qYy/nQMPHu/harTukh3\nTmGS9FxUhMutDi+N1NRU3N3dHe+Tk5Nxd3eX/z9XMNmGoRKUxw+UEFVRhw4diIiIIDIyku+++449\ne/Y4PmvSpAlJScWnwj537pyMF5QTCfpCiHKxZcsWevbsSUJCAlarFW9vb/bu3UuTJk0ue62zszNZ\nWVkVUMua51JBX/7mEkKUWt++fXnnnXeAnH2eRo0aRZcuXQCYOHEi7u7ujtlAeYe7u7ss+qpEV9zS\nV0rNAVoCK7TWk0taTinlBPyVewCM0lrvvdR3SUtfiKqrR48enD59ml27dvHUU08xYcIEAgICuP76\n62nQoAFr16695PXS0i8/ZdbSV0o9ABi11m2B65RSTa+g3E1ApNa6Q+5xyYAvhKi6pk+fzpYtW9i4\ncSM7duzg888/d+Rq3r17N/v37+fjjz8mPDwcs9nsSKZjNpvp1atXJde+ZrvS7p0OwMLc16uBdldQ\nrg3QQyn1i1JqTm7Lvwil1ONKqSilVFRcXPHz3YUQlSMvU9roxIlsGe7N7+u+ZNiwYUyaNAmTycSL\nL76IyWTi1KlTPPnkk3h5edGnTx9Hysz+/fuX63bf4vIuGfSVUhFKqQ15BzAKOJn7cQJQ+yKXuhVT\nbifQSWt9G+AMdCvuQq31TK11qNY61N+/+PSCQoiKl397ESejgRvdz3Hgi7HExZ5izJgxAERFRREe\nHl7gusWLFzta+l9//bXM2Klkl9xwTWtdYI6hUmoG4JL71p2L/9JILaZctNY6M/dcFFBs15AQomoq\nvL3IhqM2HlmSRj0vJ1q0aEFiYiJaa+Lj49mxYwe33347qamphIeHM3/+fAC+//57UlJSSEhIqKzH\nqPGutHtnFxe6dG4Gjl5BuS+UUjcrpYzAv4GrS9AqykybNm147bXXLvp5q1atGDhwIH369JHkJjVY\n4e1F6noo2jcwMvhGI++++y6HDx8mNjaWMWPGMHfuXAC+/vprR8AH6NWrFxaLBX9//yqVG6ImudKt\nlb8DNiul6gD/AtoopVoCA7XWr1yqHBANfEVOgr2lWutLD+2LCvHCCy9w5MgR3n33XXr06EFISEiR\nMh4eHri7u2OxWMjMzCzmLqImKJwprWktI+sediMWfwK7dnWcf+utty55n759+0oXTyW6opa+1jqZ\nnEHa7cA9WuskrfX+QgH/YuVitNY3aa1baa1fLnxvUfEGDRrErFmz+PXXX/n8889p374948aNA+D3\n3393JDI3mUwYjUbHUvqMjAzMZjPJycmVVndR8Qrv1wM5mdLy79cjqr4rXpyltT6ntV6otY4ti3Ki\n4n388cd4eXmxe/duDh8+zKpVq/i///s/oqKiWLBgAS4uLvzyyy/Ex8fz008/Fbn+yy+/xNvbG09P\nz0qovagsYT1HENN6MrH4Y9eKWPyJaT1ZthepZmQbhhoifyavt3eZ+CfoHhJsbtx2222OPKjR0dFk\nZGSwdu1aOnToQK9evTAYDGRlZdGiRQuysrKIjY0lLi6ONm3ayP7+Fahjx47Ur1/f0VcOYLVaMZvN\neHl5FXtNUlIS58+fx9XVtaKqKaoIyZFbwxVOjfdCqJV0vZ47F/vQuHFj3Nzc2L9/P76+vtjtdlat\nWoW7uztPP/00Q4cOLbCXv91uZ8+ePaxataoSn6jmMZvNmM3mAudMJhOenp7Ex8cDF3IQ2+127HY7\nfn5+MideFCF779QAhafaAbgoK5bU445++pYtW5KQkEBiYiJt27YFoHfv3sTGFuydMxgMHD9+HF9f\n34qpfA2WfyXrTz/9xLx58/D19cXT0xM/Pz8AlFK8+uqrBAYGYjab8fDwoHbt2jz33HOAJKMXRUnQ\nrwGKy+Rltdlx1sXve2K1WklOTiYtLQ2LxVL0fgEBZGRkyEBuOTMYDI6VrF26dGHo0KEkJCSwcuVK\njEajo9yECRM4ceIEnTt3ZtKkSRw/ftyxCZoQhUkzoAYoPNUOoN4758myK/b+73+Oc3mtdz8/P+64\n4w6++eYbx18C+XdF9PT0xG63c//99xMZGVkBT1Bz5B97wZbJzqURlx0oXbJkCU888QTnz59nw4YN\nvPbaaxL0xUXJQG4NUFwmr3RtqtCZF3PnziUrK4vhw4cDORmWfvnlFxYuXMjYsWNLtP/6ta7wv5Nl\ncjKuzopsJxfSM7MwGAy4urpis9kc+ZW9vb1JTEwEcna9bNOmDV999RXR0dH4+fk5PhM1iwzk1nBh\nPUewEwpm8mpdMZm8bDYbNpuNPXv28Ouvv9K/f3/27t3LHXfcAYCrqyv16tXjlVdeucydrn2Fx14+\n7m5h6K0mYvFneFRz6tevz8cff0xiYmKBbYvvvfdeoqOjsVqt/Pzzzzg5OdGxY8fKeARRDUjQryHC\neo6A3CAfmHtUhEWLFjFixAgMBgNaa+rWrcvYsWOpW7cuJ06cqKBaVA8BOi5nvXquobeacs/HY7fb\nyfur3Nvbm759+wKgtS52LUVeOZvNJoO5ogD5f4MoVwMGDGDx4sW0a9eOHTt24OPjQ3h4OBEREY6u\nB4PBIAu9KH7sJee8H6mpqWRkZBQ4b7PZSE5OduxjX1hSUhIZGRkFEpULIUFflKuHH36YVatW8eOP\nPzpmAyUlJREbG8vNN9/MqVOnGDhwYIFFRzXV8ZCxeBUz9nK89Vg2TSzaFefk5ERKSooEdXFFJOiL\nMpd/BsrUhrV44ou3+GbrX5w+fZp58+axc+dOdu7cybJly+jcubME/FylGXuRgC+ulAR9UaYKr/79\nLuoUL735NBl2J1CK5cuXc++99+Lh4VHZVa2SKmvsRdQcEvRFmSo8A2VkmJnpP1tpWMsZa2AIRqMR\nPz+/i/ZDX4oMSgpx9WRFrihThVf/Prk8nToeiht8bTRo0ABnZ2eWLVvGkCFDCpRbvHgxLVq0IC0t\njfnz5zt2+/zyyy/58ssv2bNnD97e3vz+++889dRTmEwmxxYFFouFf/3rXxX5mEJUWxL0RZk6owrm\nNZ7aycw791n4dI+Ne++9lw8//NAxjzw5ORmbzQaAv78/zs7OnDx5krlz5xIZGYnNZuObb75h3rx5\nJCQkMHjwYLp164bZbKZLly6OLQoGDx5cYFsCIcTFyd/KokwVnoHibTFwQ5CFDd/8j7CejwI45ueH\nhYVx6tQpVq5cyZgxY0hNTaVz586YTCZSUlLIzs5m+/bt2Gw22rRpQ8eOHTl8+DAffPBBpT2fENWd\nBH1RpkozA2X58uU0bNiQ06dP07dvX7Zt24bFYuHIkSM0btwYrTWTJk1i6tSpeHh4kJmZyerVqx17\nBaWlpUnuXiFKSPbeEUWsWbOGzp07l+t35J/W+d4eM/EN7mPpxj1ER0dTq1atAjlUtdYkJSVht9vx\n9PTkxIkTPPHEE9SvX5+lS5cyZ84c2rVrV671FaI6udTeO9KnX4PY7Xas1pxuF6vVypo1a+jTpw/3\n3XcfW7Zs4e+//wbgySef5OGHHwagYcOGeHl54erqSuvWrcukHnnTOgOJw6Dg2VszuSf1e9KSzwJg\nNBq55557HIezszMAL774Ik2aNMHb25sjR45w3XXXlUl9hKhJJOjXIGvWrKFOnTqYzWaOHTvGf/7z\nH7Zs2cKuXbsYOHAgI0bkdMH88ssv/PLLL9hsNtLS0tizZw9TpkxxBN+rVVxSF7Oy4ZyeM/NHKcX6\n9esdh91ux2az8dFHHzF79mzsdjsxMTEMGjTIcX1sbCx2u71M6ifEtUyCfg3StWtXvvjiCwYMGEBk\nZCRNmjRxZGLy9/fnt99+Y8aMGfz8888sWbKEXbt2kZ6eTuPGjYGcwLpgwYKr3iituKQuNjsY9YUu\nncTERMfh4uLC+PHjadSoEWFhYbRt25awsDA8PT1RSmG1Whk/fjzdu3e/qnoJUROUKugrpeYopbYp\npS65H65SqrZSanO+985KqWVKqa1KqWGl+W5Relarlb59+/Ldd99Rv359oqOjiYuL49y5c6SlpTFk\nyBAOHz7MU089xaOPPsqrr75KREQEAL169cLPz4/XX3+d33///arqUXhaJ0BShiZb5Uy7zN+f379/\nf2rVqsUbb7zBmjVrqFu3LvHx8Y6thUNDQ+nZsyeRkZGOvfqFEBd3xbN3lFIPAEatdVul1KdKqaZa\n64PFlPMB5gJu+U6PAnZprScqpVYqpRZprVNKXXtxWfkHTBOUP5PGPMInC9aQnp5OcHAwBw8eJCsr\ni8aNG3PTTTcxaNAgmjdvzsKFC2nUqBFDhgxh1KhRQE7rOzIy8qoHeYvbWKxLMzfqDJwM4Ej0DfD1\n1187XtepU4dVq1Zx0003Oc7Nnz//quoiRE1TmimbHYCFua9XA+2AIkEfyAb6Ad8Xuval3NebgFBg\nff6LlFKPA48DNGjQoBTVE3kK74Nz6vRp/vf1J5zHla5du+Ll5cXChQtJS0vjkUcecXTj5Pnoo4+I\njo6mX79+dOvWjdDQUPr163fV9bqapC75A74Q4spdNugrpSKAZvlO3Q3MyX2dAIQUd53WOjn3+vyn\n3YCT+a6tXcx1M4GZkDNl83L1ExdXeMA0JMgJXwuQmcHkyZNZtmwZ58+fR2vNr7/+islk4tSpUwD8\n9ddftG/fnlmzZnHHHXfw1ltv8dprr5VZ3WRjMSEqx2WDvta6QPNLKTUDcMl9686VjQuk5l6blHtt\n6hVcK65Q4UxMdrud+DRNts4mPj6eH374geXLl5OUlORIpG232zl06BAnTpzg/fffB3KSnJw5c4as\nrKzKeAwhRBkqzUDuLnK6dABuBo5W0LXiChUeMB2+LIObahv4dUwjGjVqRNeuXXnrrbeIiIjA1dUV\no9HI8OHDad++Pe+99x7PP/88vXr1Yu7cuezZs4dFixbh6urKtGnTKumJhBBX64pX5CqlPIHNwE/A\nv4A2QF1goNa6yGwepdQGrXWH3NcNgZXAWuAOoI3W+ebpFSIrcq9OgT59wGa3k6UsxLSeXKT/PG/j\ns/xbF2dkZGCxWCquwkKIMnGpFbml2oYhd2ZOZ2CT1jr2Cq+tQ05r/0etddKlykrQv3oXZu/kDpiG\nlGzAVAhRfZV50K8oEvSFEOLKyd47QgghAAn6QghRo0jQF0KIGkSCvhBC1CAS9IUQogaRoC9ECSUk\nJAAX1jQIUR1J0BfiEux2uyNj2J133slbb71Fv379CiRwadGiBe7u7nh7exc4XF1dadu2bWVVXYhi\nSdAX4hIWLFjgeH3q1ClGjhxJZGQk27dvd2wB7eTkxA8//FAg8UtiYiKffPJJmWUbE6KslGZrZSFq\njHHjxhEbG4uPjw8pKSkFtvtu0qQJP//8M0opunXrhsFgICUlBS8vLxITE/H29ub++++vxNoLUZS0\n9IW4hJ9//pnk5GQGDx5M7969SUhIcBxJSUnccMMNGAwGRowYQZs2bXBxceHee+/FZDLRsWNH7rvv\nvsp+BCEKkJa+EIXkzzZmUP7svPE5Zs+ejdFoxNfXF4CsrCx2795N06ZNsdvttGrVipYtW7J3716e\neOIJ1q5dy4gRI6hXrx52ux2DQdpXomqQvXeEyKfwzqQA7T9L448UN+o2aMSePXtYunQp//3vf/nt\nt98AaNasGfHx8SQlJeHt7c25c+fw8/PDZrNhNBr54IMPePDBByvrkUQNJHvvCFFChbON9VlwnsMJ\n2fz2tC/u7u7cf//9jBw5khUrVjjKnDlzhmPHjtGoUSO2bt3KLbfcwqRJkzCbzUyZMqXGB/yPP/6Y\ndevWAfDQQw9x+PDhSq5RzSZBX4h8AnSc4/Wc3ZkcOqf5a7QbhvPx+Pn5sXLlSgwGA8ePHwdyAr7W\nGnd3d7TWvP7664SHhzNmzBgCAgIYPnx4ZT1KubNarcWet9lsZGRkON6/+uqrREdHExUVxaJFi6hb\nt25FVVEUQ4K+EPnkzzb2aIiZHcNdafTueVp9cp6goCDS09MZPnw4vXv3xsXFhTfeeIM777wTyFm8\nFR0dzSOPPMKhQ4ew2+24u7uzdOnSynqccnXDDTc41iQ4OztjNpvx9vbG19fX8d8kJiaGlJQUnn32\nWV555RVsNht16tTB19cXX19fXnjhhUp+ippH+vSFyKe4Pv2T55041X5KsdnGnJycSEtLw9XVlRMn\nTlCvXr0CZeLjc/5CuNZ16tSJRo0aMXv27ALn27dvz759+4iJieH6668nNDSUf//73zz77LPY7XZs\nNhsmk6mSan3tkj59IUoorOcIYlpPJhZ/7FoRi3+xAR8upJZ0dXUFKBLwgRoR8C8mOjqaHTt2APDw\nww/Tq1evAuk409LSJOBXAmnpCyGuWnEt/a5du9KsWTPmz5/PoUOHcHV1pUePHmzduhWz2YzRaOTs\n2bOVWOtrl7T0hRBXbefSCGInBmOf4EXsxGB2Lo24ZPlVq1bx1FNPAeDr64vFYgFg6tSpJCYmSsCv\nJBL0hRCXlTfWEUgcBgWBxHHjrlcuGfgvtiBt3LhxeHt74+HhweLFi8uryuIiJOgLIS6r8PqF++af\nx39qPB3Dn8TT05ONGzfyxRdf4OnpiaenJ+7u7nTv3h273V7kXnkt/ZSUFPr06VORjyEo5TYMSqk5\nQEtghdZ68iXK1Qa+0Vq3z31fF9gBHMotEq51vonRQogqKUDHgbrw/ofBbgDYtcLwauJFr9u9ezfZ\n2dmO9zab7aLz+0XFkawrSAAABU1JREFUuOKgr5R6ADBqrdsqpT5VSjXVWh8sppwPMBdwy3f6duB1\nrfXHpa6xEKLCnVH+BFK0fXZG+RF4ietCQkJISkpyvN+wYUPZV05ckdJ073QAFua+Xg20u0i5bKAf\nkJzvXBtguFJqt1JqSnEXKaUeV0pFKaWi4uLkjwAhqoLjIWNJ1wWnV6ZrE8dDxlZSjURpXTboK6Ui\nlFIb8g5gFHAy9+MEoHZx12mtk7XWSYVOryLnl0YY0FYpdVMx183UWodqrUP9/f0LfyyEqATFrV+I\naT252PULomq7bPeO1rrAv6pSagbgkvvWnSv7a+FnrXVm7n1+hf9v735C46rCMA7/XmIFW7UoSm1d\nFIRuijaraoIuRrFBRURUEHQnQhFacCsIIqjLQhFEBRfiwoULBRcldWFI8Q/aKKYVdVdFIVixtLhR\nkM/F3DpjnJs5c5PMPZPzPnDJhJzLvF9OzpeZO3fusA9YHmF/M2vJwYcOQ9Xkb6o2mzxNDu8s0Tuk\nMw2cG2HfeUm7JW0H5oCzDe7fzMwaanL2zgfAKUl7gPuBGUn7gSci4vkh+74IfAz8BbweET80uH8z\nM2uo0WUYqjNzDgGLEbGy4akqvgyDmdno1roMQ6Pz9CPiAr0zeMzMbEL4HblmZgVx0zczK4ibvplZ\nQdz0zcwK4qZvZlYQN30zs4K46ZuZFcRN38ysIG76ZmYFaXQZhnGRdB74cUx3dwPw25jua7NshRrA\ndeTGdeQjtYa9ETHw2vRZN/1xknS67loVk2Ir1ACuIzeuIx8bUYMP75iZFcRN38ysIG76PW+2HWAD\nbIUawHXkxnXkY901+Ji+mVlB/EjfzKwgbvpmZgUpsulL2iXp1JAx2yR9KOkTSU+NK9soJL0l6TNJ\ntZ9NLOkKST9JWqi228aZcS2J+YeOaduwjDnPQb9h62IS1gQk1XGzpJ/75mPg+extkbRT0glJJyW9\nL+nKmnGN1kZxTb/6fN+3gR1Dhh4FliLiTuAxSddsergRSHoEmIqIWeAWSftqhh4A3o2ITrWdGV/K\nein5R6ixNYkZs5yDfonrIus1Acl13AG83Dcf58eTLtmTwLGImANWgPtWD1jP2iiu6QN/A48Dl4aM\n69D7HOBFILc3dXTo5TsJ3FUzbgZ4UNIX1SODRp+LvAk6DM+fMqZtHYZnzHUO+qWsiw55rwlIq2MG\neFrSV5JeGU+sdBHxWkR8VH17I/DrgGEdGq6NLd/0Jb3R9zRuAXg2Ii4m7LoD+KW6/Tuwa7MyphhQ\nx1HS8n0J3BsRtwPbgAc2PWyalN9vVnNQIyVjrnPwr4i4lLAusp+PxDpO0G2aB4FZSQc2PVgDkmaB\n6yLi8wE/bjwXOT7i2FARcbjhrn8AVwEXgaur71uzug5Jx+nmg26+un/gyxHxZ3X7NJDLIZLLv1+o\nz58ypm0pGXOdg1FltSbW4dPL8yHpa7rzsdxupP+SdD3wKvBozZDGayPHRZSLJXpPmaaBc+1FGSg1\n3zuSpiVNAQ8D34whW4qU/LnPAaRlzHUORjUJ85FiXtJuSduBOeBs24H6VS/cvgc8FxF1F5xsPhcR\nUeQGLPTdvgc4surne4FvgeN0n55PtZ15Vb5r6TaPY8B3wE5gP/DSqnG30n0Uc4bui1etZ6/JPz0g\n+/9qbDt3wzqynIOaehaqrxO3Jkao427g+2pOjow7W0L2Z4ALwEK1vbCRa8PvyF2DpD10/5vOR9rr\nAGNVnalwCFiMiJW284wqJf8k1DgJGTdK7muiJE3/7tz0zcwK4mP6ZmYFcdM3MyuIm76ZWUHc9M3M\nCuKmb2ZWkH8A78n1otHSyLQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "words = idx_to_word[:30]\n",
    "X = embedding_weights[:30]\n",
    "X_2 = PCA(n_components=2).fit_transform(X)\n",
    "print(X.shape, X_2.shape)\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n",
    "plt.scatter(X_2[:, 0], X_2[:, 1])  # 散点图的xy坐标\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(X_2[i, 0], X_2[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单词之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'man'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-618b62507fed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mman_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"man\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mking_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"king\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwoman_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"woman\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwoman_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0membedding_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mman_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0membedding_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mking_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcos_dis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0membedding_weights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'man'"
     ]
    }
   ],
   "source": [
    "man_idx = word_to_idx[\"man\"] \n",
    "king_idx = word_to_idx[\"king\"] \n",
    "woman_idx = word_to_idx[\"woman\"]\n",
    "embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]\n",
    "cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "for i in cos_dis.argsort()[:20]:\n",
    "    print(idx_to_word[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
